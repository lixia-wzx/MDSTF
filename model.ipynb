{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import weight_norm\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED=5\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_datasets = r\"/root/data_apolloscape/train_data_119.pkl\"\n",
    "test_datasets = r\"/root/data_apolloscape/test_data_119.pkl\"\n",
    "model_save_path = r\"/root/data_apolloscape/model_save/\"\n",
    "max_object_num = 115\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "dropout = 0.2\n",
    "in_channels = 6\n",
    "out_channels = 2\n",
    "hidden_size = 64\n",
    "heads = 8\n",
    "layers = 5\n",
    "history_frames = 6\n",
    "future_frames = 6\n",
    "kernel_size = [2,3]\n",
    "paddings = [[1,2,2],[2,2,2]]\n",
    "dilations = [[1,2,2],[1,1,1]]\n",
    "\n",
    "# kernel_size = [2,5]\n",
    "# paddings = [[1,2,2],[4,4]]\n",
    "# dilations = [[1,2,2],[1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Feeder(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 data_cache,\n",
    "                 train_percent=0.8,\n",
    "                 train_val_test='train'):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.data_cache = data_cache\n",
    "        self.train_val_test = train_val_test\n",
    "\n",
    "        self.load_data()\n",
    "\n",
    "        total_num = len(self.all_data)\n",
    "        # equally choose validation set\n",
    "        train_id_list = list(np.linspace(0, total_num-1, int(total_num*train_percent)).astype(int))\n",
    "        val_id_list = list(set(list(range(total_num))) - set(train_id_list))\n",
    "\n",
    "        # # last 20% data as validation set\n",
    "        if train_val_test.lower() == 'train':\n",
    "            self.all_data = self.all_data[train_id_list]\n",
    "        elif train_val_test.lower() == 'val':\n",
    "            self.all_data = self.all_data[val_id_list]\n",
    "\n",
    "    def load_data(self):\n",
    "\n",
    "        with open(self.data_cache, 'rb') as reader:\n",
    "            [self.all_data] = pickle.load(reader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = self.all_data[idx].copy()\n",
    "\n",
    "        if self.train_val_test.lower() == 'train':\n",
    "            th = np.random.random() * np.pi * 2\n",
    "            data['features'][:, :, 0] = data['features'][:, :, 0] * np.cos(th) - data['features'][:, :, 0] * np.sin(th)\n",
    "            data['features'][:, :, 1] = data['features'][:, :, 1] * np.sin(th) + data['features'][:, :, 1] * np.cos(th)\n",
    "            \n",
    "        if self.train_val_test.lower() == 'test':\n",
    "            return data['features'],data['masks'],data['origin'],data['distance_adj'],data['heading_adj'],data[\"mean\"]\n",
    "        else:\n",
    "            return data['features'],data['masks'],data['distance_adj'],data['heading_adj'],data[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainLoader = Feeder(r\"/kaggle/input/\",train_datasets, 0.8, 'train')\n",
    "train_loader = DataLoader(dataset=trainLoader, batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "valLoader = Feeder(r\"/kaggle/input/\", train_datasets, 0.8, 'val')\n",
    "val_loader = DataLoader(dataset=valLoader,batch_size=batch_size,shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lap(adj):\n",
    "    adj = torch.nan_to_num(adj / torch.sum(adj, dim=-1).unsqueeze(dim=-1), nan=0)\n",
    "    return adj\n",
    "\n",
    "\n",
    "# def get_lap(adj):\n",
    "#     # (64,6,115,115)\n",
    "#     batch,step,num_object = adj.shape[0], adj.shape[1],adj.shape[2]\n",
    "#     adj = adj.reshape(batch*step,num_object,num_object) #(64*6,115,115)\n",
    "#     D = torch.sum(adj, dim=-1)\n",
    "#     D = D**(-0.5)\n",
    "#     D[D==torch.inf]=0\n",
    "#     D = torch.diag_embed(D)\n",
    "#     lap = torch.matmul(torch.matmul(D,adj),D)\n",
    "#     lap = lap.reshape(batch,step,num_object,num_object)\n",
    "#     return lap\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self._linear1 = nn.Linear(d_model, d_ff)\n",
    "        self._linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._linear2(F.relu(self._linear1(x)))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # 创建位置编码矩阵\n",
    "        pe = torch.zeros(self.max_seq_len, self.d_model).cuda()\n",
    "\n",
    "        # 计算位置编码的值\n",
    "        position = torch.arange(0, self.max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * (-math.log(10000.0) / self.d_model))\n",
    "\n",
    "        # 调整位置编码矩阵的值\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 添加一个维度作为可学习的参数\n",
    "        pe = pe.unsqueeze(0)  # (1,6,h)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将位置编码加到输入张量中\n",
    "        # (32,115,6,h)\n",
    "        x = x + self.pe[:, :x.shape[2]]\n",
    "        return x\n",
    "\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_seq_len, period=6):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.max_seq_len = max_seq_len\n",
    "\n",
    "#         # 创建位置编码矩阵\n",
    "#         pe = torch.zeros(self.max_seq_len, self.d_model).cuda()\n",
    "\n",
    "#         pos = torch.arange(self.max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "#         pe = torch.sin(pos * 2 * np.pi / period)\n",
    "#         pe = pe.repeat((1, d_model))\n",
    "\n",
    "#         # 添加一个维度作为可学习的参数\n",
    "#         pe = pe.unsqueeze(0)  # (1,6,h)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # 将位置编码加到输入张量中\n",
    "#         # (32,115,6,h)\n",
    "#         x = x + self.pe[:, :x.shape[2]]\n",
    "#         return x\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        其实这就是一个裁剪的模块，裁剪多出来的padding\n",
    "        \"\"\"\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, padding, dilation, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = weight_norm(\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=(kernel_size, 1),\n",
    "                      padding=(padding, 0),\n",
    "                      stride=1,\n",
    "                      dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=(kernel_size, 1),\n",
    "                      padding=(padding, 0),\n",
    "                      stride=1,\n",
    "                      dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)  # 裁剪掉多出来的padding部分，维持输出时间步为seq_len\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        # self.residual =  nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=(1, 1))\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        参数初始化\n",
    "        \"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        # self.residual.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:(64,T,114,32)\n",
    "        x = x.transpose(2, 3).transpose(1, 2)  # (64,32,6,114)\n",
    "        out = self.net(x)  # (64,32,6,114)\n",
    "        out = out + x\n",
    "        out = out.transpose(1, 2).transpose(2, 3)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, hidden_size, paddings, dilations, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(dilations)):\n",
    "            dilation_size = dilations[i]  # 膨胀系数：1，2，2\n",
    "            padding_size = paddings[i]\n",
    "            layers += [\n",
    "                TCNBlock(hidden_size=hidden_size, dilation=dilation_size, padding=padding_size, kernel_size=kernel_size,\n",
    "                         dropout=dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:(64,T,114,32)\n",
    "        return self.network(x)\n",
    "    \n",
    "\n",
    "class Matmul(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        x = torch.matmul(A, x)\n",
    "        return x.contiguous()\n",
    "\n",
    "\n",
    "class DiffusionConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, dropout, support_len=2, order=2):\n",
    "        super().__init__()\n",
    "        self.nconv = Matmul()\n",
    "        c_in = (order * support_len + 1) * c_in\n",
    "        self.mlp = nn.Linear(c_in, c_out)\n",
    "        self.dropout = dropout\n",
    "        self.order = order\n",
    "\n",
    "    def forward(self, x, dist_adj, heading_adj):\n",
    "        support = [dist_adj,heading_adj]\n",
    "        out = [x]\n",
    "        for a in support:\n",
    "            x1 = self.nconv(x, a)\n",
    "            out.append(x1)\n",
    "            for k in range(2, self.order + 1):\n",
    "                x2 = self.nconv(x1, a)\n",
    "                out.append(x2)\n",
    "                x1 = x2\n",
    "\n",
    "        h = torch.cat(out, dim=-1)\n",
    "        h = self.mlp(h)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        return h\n",
    "\n",
    "\n",
    "class GlobalSpatialMHA(nn.Module):\n",
    "    def __init__(self, heads, hidden_size):\n",
    "        super().__init__()\n",
    "        self.d_v = hidden_size // heads\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_q, input_k, input_v):\n",
    "        # x(64,115,h)\n",
    "        batch, num_object = input_v.shape[0], input_v.shape[1]\n",
    "        Q = self.W_Q(input_q).reshape(batch, num_object, self.heads, self.d_v).transpose(1, 2)  # (64, heads, 115, d_v)\n",
    "        K = self.W_K(input_k).reshape(batch, num_object, self.heads, self.d_v).transpose(1, 2)  # (64, heads, 115, d_v)\n",
    "        V = self.W_V(input_v).reshape(batch, num_object, self.heads, self.d_v).transpose(1, 2)  # (64, heads, 115, d_v)\n",
    "        attention = torch.matmul(Q, K.transpose(-1, -2)) / (self.d_v ** 0.5)  # (64, heads, 115,115)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        context = torch.matmul(attention, V)  # (64, heads, 115,h)\n",
    "        context = context.transpose(1, 2)\n",
    "        context = context.reshape(batch, num_object, self.heads * self.d_v)  # (64, 115, heads*d_v)\n",
    "        context = self.fc(context)  # x:(64,115,h)\n",
    "        return context\n",
    "\n",
    "\n",
    "class SpatialMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, hidden_size, history_frames):\n",
    "        super().__init__()\n",
    "        self.d_v = hidden_size // heads\n",
    "        self.heads = heads\n",
    "        self.history_frames = history_frames\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_q, input_k, input_v):\n",
    "        # x:(64,6,115,h)\n",
    "        batch, num_object = input_q.shape[0], input_q.shape[2]\n",
    "        Q = self.W_Q(input_q).reshape(batch, self.history_frames, num_object, self.heads, self.d_v).transpose(2, 3)\n",
    "        K = self.W_K(input_k).reshape(batch, self.history_frames, num_object, self.heads, self.d_v).transpose(2, 3)\n",
    "        V = self.W_V(input_v).reshape(batch, self.history_frames, num_object, self.heads, self.d_v).transpose(2, 3)\n",
    "        attention = torch.matmul(Q, K.transpose(-1, -2)) / (self.d_v ** 0.5)  # (64, 6, heads, 115,115)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        context = torch.matmul(attention, V)  # (64, 6, heads, 115,h)\n",
    "        context = context.transpose(2, 3)\n",
    "        context = context.reshape(batch, self.history_frames, num_object,\n",
    "                                  self.heads * self.d_v)  # (64, 6, 115, heads*h)\n",
    "        context = self.fc(context)  # x:(64,6,115,h)\n",
    "        # context = self.dropout(context)  # x:(64,115,6,h)\n",
    "        return context\n",
    "\n",
    "\n",
    "class TemporalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, hidden_size, history_frames, dropout):\n",
    "        super().__init__()\n",
    "        self.d_v = hidden_size // heads\n",
    "        self.heads = heads\n",
    "        self.history_frames = history_frames\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.masks = torch.tril(torch.ones((history_frames, history_frames)), diagonal=0).cuda()\n",
    "        self.layerNorm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layerNorm2 = nn.LayerNorm(hidden_size)\n",
    "        self.feedForward = FeedForward(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_q, input_k, input_v):\n",
    "        residual = input_q\n",
    "        batch, num_object = input_q.shape[0], input_q.shape[1]\n",
    "        Q = self.W_Q(input_q).reshape(batch, num_object, self.history_frames, self.heads, self.d_v).transpose(2, 3)\n",
    "        K = self.W_K(input_k).reshape(batch, num_object, self.history_frames, self.heads, self.d_v).transpose(2, 3)\n",
    "        V = self.W_V(input_v).reshape(batch, num_object, self.history_frames, self.heads, self.d_v).transpose(2, 3)\n",
    "        attention = torch.matmul(Q, K.transpose(-1, -2)) / (self.d_v ** 0.5)  # (64, 115, heads, 6, 6)\n",
    "        context = torch.matmul(F.softmax(attention, dim=-1), V)  # (64, 115, heads, 6,h)\n",
    "        context = context.transpose(2, 3)\n",
    "        context = context.reshape(batch, num_object, self.history_frames,\n",
    "                                  self.heads * self.d_v)  # (64, 115, 6, heads*h)\n",
    "        context = self.fc(context)  # x:(64,115,6,h)\n",
    "        context = self.dropout(context)  # x:(64,115,6,h)\n",
    "        context = self.layerNorm1(context + residual)  # (64,6,114,h)\n",
    "\n",
    "        context_1 = self.feedForward(context)  # (64,6,114,h)\n",
    "        context_1 = self.dropout(context_1)  # (64,6,114,h)\n",
    "        last_out = self.layerNorm2(context_1 + context)  # (64,6,114,h)\n",
    "        last_out = last_out.transpose(1, 2)\n",
    "        return last_out\n",
    "\n",
    "\n",
    "class GlobalSpatial(nn.Module):\n",
    "    def __init__(self, heads, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.gsa = GlobalSpatialMHA(heads, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x(64,T,114,h)\n",
    "        batch, step, num_object = x.shape[0], x.shape[1], x.shape[2]\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x.reshape(batch * num_object, step, self.hidden_size)  # x(64*115,T,h)\n",
    "        x,gt = self.gru(x)[1][0],self.gru(x)[0]  # (1,64*115,h)\n",
    "        x = x.squeeze().reshape(batch, num_object, self.hidden_size)  # (64,115,h)\n",
    "        x = self.gsa(x, x, x)  # (64,115,h)\n",
    "        x = torch.unsqueeze(x, dim=1)  # (64,1,115,h)\n",
    "        gt = gt.reshape(batch, num_object, step, self.hidden_size).transpose(1,2)\n",
    "        return x,gt # (64,1,115,h)\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, heads, history_frames, hidden_size, paddings, dilations, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        self.gt = TemporalMultiHeadAttention(heads, hidden_size, history_frames, dropout)\n",
    "        # self.pe = PositionalEncoding(hidden_size, 32)\n",
    "        # self.fc_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc_2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # (32,115,6,h)\n",
    "        # x = self.pe(x)\n",
    "\n",
    "        Q, K, V = x, x, x\n",
    "        tmha = self.gt(Q, K, V)\n",
    "        \n",
    "        # z = torch.sigmoid(self.fc_1(tmha) + self.fc_2(gt))\n",
    "        # last_out = z * tmha + (1 - z) * gt\n",
    "        return tmha\n",
    "\n",
    "\n",
    "class SpatialBlock(nn.Module):\n",
    "    def __init__(self, heads, hidden_size, history_frames, dropout):\n",
    "        super().__init__()\n",
    "        self.satt = SpatialMultiHeadAttention(heads, hidden_size, history_frames)\n",
    "        self.dc = DiffusionConv(hidden_size, hidden_size, dropout)\n",
    "        self.fc_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_4 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, dist_adj, heading_adj, gs_out):\n",
    "        # (64,6,3,115,115)\n",
    "        Q, K, V = x, x, x\n",
    "        satt_out = self.satt(Q, K, V)\n",
    "        gcn_out = self.dc(x, dist_adj, heading_adj)\n",
    "        \n",
    "        z = torch.sigmoid(self.fc_1(gcn_out) + self.fc_2(satt_out))\n",
    "        last_out = z * gcn_out + (1 - z) * satt_out\n",
    "\n",
    "        a = torch.sigmoid(self.fc_3(last_out) + self.fc_4(gs_out))\n",
    "        last_out = a * last_out + (1 - a) * gs_out\n",
    "        return last_out\n",
    "\n",
    "\n",
    "class SpatialTemporal(nn.Module):\n",
    "    def __init__(self, hidden_size, history_frames, heads, paddings, dilations, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        self.gs = GlobalSpatial(heads, hidden_size)\n",
    "        self.spatial = SpatialBlock(heads, hidden_size, history_frames, dropout)\n",
    "        self.temporal = TemporalBlock(heads, history_frames, hidden_size, paddings, dilations, kernel_size, dropout)\n",
    "\n",
    "    def forward(self, x, dist_adj, heading_adj):\n",
    "        # x:(64,T,114,h), (64,6,115,115), (64,6,115,115)\n",
    "        gs_out,gt = self.gs(x)\n",
    "        x = self.spatial(x, dist_adj, heading_adj, gs_out)\n",
    "        x = self.temporal(x)\n",
    "        x = x + gt\n",
    "        return x\n",
    "\n",
    "\n",
    "# class Seq2Seq(nn.Module):\n",
    "#     def __init__(self, hidden_size, out_channels, history_frames, future_frames, max_object_num):\n",
    "#         super().__init__()\n",
    "#         self.out_channels = out_channels\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.future_frames = future_frames\n",
    "#         self.max_object_num = max_object_num\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "#         self.gru_cell = nn.GRU(hidden_size + hidden_size, hidden_size, batch_first=True)\n",
    "#         self.W_e = nn.Linear(hidden_size,hidden_size)\n",
    "#         self.W_d = nn.Linear(hidden_size,hidden_size)\n",
    "#         self.W_a = nn.Linear(hidden_size, 1)\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "#         self.fc_2 = nn.Linear(hidden_size, out_channels)\n",
    "# #         self.reset_parameters()\n",
    "\n",
    "# #     def reset_parameters(self):\n",
    "# #         nn.init.xavier_uniform_(self.W_e)\n",
    "# #         nn.init.xavier_uniform_(self.W_d)\n",
    "# #         nn.init.xavier_uniform_(self.W_a)\n",
    "\n",
    "#     def forward(self, h, last_position, teacher_location=None):\n",
    "#         # (64,6,114,h), (64,114,2)\n",
    "#         if teacher_location is not None:\n",
    "#             teacher_location = teacher_location.transpose(1, 2)\n",
    "#             teacher_location = teacher_location.reshape(-1, teacher_location.shape[2], 2)  # (64*114,6,2)\n",
    "#         h = h.transpose(1, 2)\n",
    "#         h = h.reshape(-1, h.shape[2], self.hidden_size)  # (64*114, 6, h)\n",
    "#         last_position = last_position.reshape(-1, 2).unsqueeze(dim=1)  # (64*114, 1, 2)\n",
    "#         last_out = torch.zeros((h.shape[0], self.future_frames, 2)).cuda()  # (64*114,6,2)\n",
    "\n",
    "#         x = torch.zeros((h.shape[0], 1, self.hidden_size)).cuda()  # (64*114,1,h)\n",
    "#         x = torch.cat([last_position, x], dim=-1)  # (64*114,1,h+2)\n",
    "\n",
    "#         output, h_t = self.gru(h)  # (64*114,6,h), (1,64*114,h)\n",
    "#         for step in range(self.future_frames):\n",
    "#             if step == 0:\n",
    "#                 new_out, h_t = self.gru_cell(x, h_t)  # (64*114,1,h), (1,64*114,h)\n",
    "#                 # new_out =   # (64*114, 1, 2)\n",
    "#                 # new_out = new_out + last_position  # (64*114, 1, 2)\n",
    "#                 last_out[:, step:step + 1, :] = self.fc_2(new_out)\n",
    "#             else:\n",
    "#                 a = F.softmax(self.W_a(torch.tanh(self.W_e(output) + self.W_d(h_t.transpose(0, 1)))),dim=1)  # (64*114,6,1)\n",
    "#                 c = torch.matmul(output.transpose(1, 2), a).transpose(1, 2)  # (64*114,1,h)\n",
    "#                 teacher_force = np.random.random() < 0.5\n",
    "#                 new_out = (teacher_location[:, step - 1:step] if (type(teacher_location) is not type(\n",
    "#                     None)) and teacher_force else new_out)\n",
    "#                 new_out, h_t = self.gru_cell(torch.cat([new_out, c], dim=-1), h_t)  # (64*114,1,h), (64*114,1,h)\n",
    "#                 last_out[:, step:step + 1, :] = self.fc_2(new_out)\n",
    "#         last_out = last_out.reshape(-1, self.max_object_num, self.future_frames, self.out_channels)\n",
    "#         last_out = last_out.transpose(1, 2)\n",
    "#         return last_out\n",
    "\n",
    "\n",
    "# class Seq2Seq(nn.Module):\n",
    "#     def __init__(self, hidden_size, out_channels, history_frames, future_frames, max_object_num):\n",
    "#         super().__init__()\n",
    "#         self.out_channels = out_channels\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.future_frames = future_frames\n",
    "#         self.max_object_num = max_object_num\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "#         self.gru_cell = nn.GRU(hidden_size + hidden_size, hidden_size, batch_first=True)\n",
    "#         self.fc = nn.Linear(out_channels,hidden_size)\n",
    "#         self.W_e = nn.Linear(hidden_size,hidden_size)\n",
    "#         self.W_d = nn.Linear(hidden_size,hidden_size)\n",
    "#         self.W_a = nn.Linear(hidden_size, 1)\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "#         self.fc_2 = nn.Linear(hidden_size, out_channels)\n",
    "\n",
    "#     def forward(self, h, last_position, teacher_location=None):\n",
    "#         # (64,6,114,h), (64,114,2)\n",
    "#         if teacher_location is not None:\n",
    "#             teacher_location = teacher_location.transpose(1, 2)\n",
    "#             teacher_location = teacher_location.reshape(-1, teacher_location.shape[2], 2)  # (64*114,6,2)\n",
    "#             teacher_location = self.fc(teacher_location)\n",
    "#         h = h.transpose(1, 2)\n",
    "#         h = h.reshape(-1, h.shape[2], self.hidden_size)  # (64*114, 6, h)\n",
    "#         last_position = last_position.reshape(-1, 2).unsqueeze(dim=1)  # (64*114, 1, 2)\n",
    "#         last_position = self.fc(last_position)\n",
    "#         last_out = torch.zeros((h.shape[0], self.future_frames, 2)).cuda()  # (64*114,6,2)\n",
    "\n",
    "#         x = torch.zeros((h.shape[0], 1, self.hidden_size)).cuda()  # (64*114,1,h)\n",
    "#         x = torch.cat([last_position, x], dim=-1)  # (64*114,1,h+2)\n",
    "\n",
    "#         output, h_t = self.gru(h)  # (64*114,6,h), (1,64*114,h)\n",
    "#         for step in range(self.future_frames):\n",
    "#             if step == 0:\n",
    "#                 new_out, h_t = self.gru_cell(x, h_t)  # (64*114,1,h), (1,64*114,h)\n",
    "#                 last_out[:, step:step + 1, :] = self.fc_2(new_out)\n",
    "#             else:\n",
    "#                 a = F.softmax(self.W_a(torch.tanh(self.W_e(output) + self.W_d(h_t.transpose(0, 1)))),dim=1)  # (64*114,6,1)\n",
    "#                 c = torch.matmul(output.transpose(1, 2), a).transpose(1, 2)  # (64*114,1,h)\n",
    "#                 teacher_force = np.random.random() < 0.5\n",
    "#                 new_out = (teacher_location[:, step - 1:step] if (type(teacher_location) is not type(\n",
    "#                     None)) and teacher_force else new_out)\n",
    "#                 new_out, h_t = self.gru_cell(torch.cat([new_out, c], dim=-1), h_t)  # (64*114,1,h), (64*114,1,h)\n",
    "#                 last_out[:, step:step + 1, :] = self.fc_2(new_out)\n",
    "#         last_out = last_out.reshape(-1, self.max_object_num, self.future_frames, self.out_channels)\n",
    "#         last_out = last_out.transpose(1, 2)\n",
    "#         return last_out\n",
    "    \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads, hidden_size, layers, history_frames, max_object_num, paddings,\n",
    "                 dilations, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(in_channels, hidden_size)\n",
    "        self.layers = layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.history_frames = history_frames\n",
    "        self.max_object_num = max_object_num\n",
    "        self.st_block = SpatialTemporal(hidden_size, history_frames, heads, paddings, dilations, kernel_size, dropout)\n",
    "        self.tcn_1 = TCN(hidden_size, paddings[0], dilations[0], kernel_size[0], dropout)\n",
    "        self.tcn_2 = TCN(hidden_size, paddings[1], dilations[1], kernel_size[1], dropout)\n",
    "        self.fc_1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_2 = nn.Linear(hidden_size, out_channels)\n",
    "        # self.seq2seq = Seq2Seq(hidden_size,out_channels,history_frames,history_frames,max_object_num)\n",
    "\n",
    "    def forward(self, x, dist_adj, heading_adj):\n",
    "        # last_position = x[:,-1,:,:2]\n",
    "        x = self.embed(x)  # (32,6,115,h)\n",
    "        residual = x  # (64,T,115,h)\n",
    "\n",
    "        dist_adj = get_lap(dist_adj)\n",
    "        heading_adj = get_lap(heading_adj)\n",
    "     \n",
    "        for layer in range(self.layers):\n",
    "            x = self.st_block(x, dist_adj, heading_adj) + residual # (64,6,115,h)\n",
    "            residual = x\n",
    "        # last_out = self.seq2seq(x,last_position,teacher_location)\n",
    "        # last_out = self.tcn_1(x)\n",
    "        last_out = torch.cat([self.tcn_1(x),self.tcn_2(x)],dim=-1)\n",
    "        last_out = self.fc_1(last_out)\n",
    "        last_out = self.fc_2(last_out)\n",
    "        return last_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(in_channels, out_channels, heads, hidden_size, layers, history_frames, max_object_num, paddings, dilations, kernel_size, dropout)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, threshold=0.01,threshold_mode=\"abs\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inverse_transform(prediction, last_position):\n",
    "    # (64,6,115,2),(64,115,2)\n",
    "    for step in range(prediction.shape[1]):\n",
    "        prediction[:,step,:,:] = prediction[:,step,:,:] + last_position\n",
    "        last_position = prediction[:,step,:,:]\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def eva(predicted, ground_truth, future_masks):\n",
    "    #(64,6,114,2),(64,6,114,8),(64,6,114,1)\n",
    "    predicted = predicted.transpose(2,3).transpose(1,2)  # (64,2,6,114)\n",
    "    ground_truth = ground_truth.transpose(2,3).transpose(1,2)  # (64,8,6,114)\n",
    "    future_masks = future_masks.transpose(2,3).transpose(1,2)  # (64,1,6,114)\n",
    "    \n",
    "    category_mask = ground_truth[:, 2:3, :, :]  # (N, C, T, V)=(64, 1, 6, 114)\n",
    "    \n",
    "    ### overall dist\n",
    "    overall_sum_time, overall_num = compute_RMSE(predicted, ground_truth[:,-2:,:,:], future_masks)\n",
    "\n",
    "    ### car dist\n",
    "    car_mask = (((category_mask == 1) + (category_mask == 2)) > 0).float().to(device)\n",
    "    car_mask = future_masks * car_mask\n",
    "    car_sum_time, car_num = compute_RMSE(predicted, ground_truth[:,-2:,:,:], car_mask)\n",
    "\n",
    "    ### human dist\n",
    "    human_mask = (category_mask == 3).float().to(device)\n",
    "    human_mask = future_masks * human_mask\n",
    "    human_sum_time, human_num = compute_RMSE(predicted, ground_truth[:,-2:,:,:], human_mask)\n",
    "\n",
    "    ### bike dist\n",
    "    bike_mask = (category_mask == 4).float().to(device)\n",
    "    bike_mask = future_masks * bike_mask\n",
    "    bike_sum_time, bike_num = compute_RMSE(predicted, ground_truth[:,-2:,:,:], bike_mask)\n",
    "    \n",
    "    return overall_num,overall_sum_time, car_num,car_sum_time, human_num,human_sum_time,bike_num,bike_sum_time\n",
    "\n",
    "\n",
    "def compute_RMSE(predicted, ground_truth, masks, error_order=2):\n",
    "    predicted = predicted * masks  # (N, C, T, V)=(N, 2, 6, 114)\n",
    "    ground_truth = ground_truth * masks  # (N, C, T, V)=(N, 2, 6, 114)\n",
    "\n",
    "    x2y2 = torch.sum(torch.abs(predicted - ground_truth) ** error_order, dim=1)  # x^2+y^2, (N, C, T, V)->(N, T, V)=(64, 6, 114)\n",
    "    total_sum_time = x2y2.sum(dim=-1)  # (N, T, V) -> (N, T)=(64, 6)\n",
    "    total_mask = masks.sum(dim=1).sum(dim=-1)  # (N, C, T, V) -> (N, T)=(N, 6)\n",
    "\n",
    "    return total_sum_time.detach().cpu().numpy(), total_mask.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def display_result(pra_results, pra_pref='Train_epoch'):\n",
    "    # all_overall_sum_list,all_overall_num_list:(num_batch*batch_size,6)\n",
    "    all_overall_sum_list, all_overall_num_list = pra_results\n",
    "    overall_sum_time = np.sum(all_overall_sum_list ** 0.5, axis=0)\n",
    "    overall_num_time = np.sum(all_overall_num_list, axis=0)\n",
    "    overall_loss_time = (overall_sum_time / overall_num_time)\n",
    "    return overall_loss_time\n",
    "\n",
    "\n",
    "def show_result(result_car,result_human,result_bike,stage=\"val\"):\n",
    "    result = 0.20 * result_car + 0.58 * result_human + 0.22 * result_bike\n",
    "    WSADE = np.sum(result)/6\n",
    "    ADE_v = np.sum(result_car)/6\n",
    "    ADE_p = np.sum(result_human)/6\n",
    "    ADE_b = np.sum(result_bike)/6\n",
    "    \n",
    "    WSFDE = result[-1]\n",
    "    FDE_v = result_car[-1]\n",
    "    FDE_p = result_human[-1]\n",
    "    FDE_b = result_bike[-1]\n",
    "    \n",
    "    if stage==\"val\":\n",
    "        log = 'val ADEv: {:.4f}, val ADEp: {:.4f}, val ADEb: {:.4f}, val WSADE: {:.4f}, val FDEv: {:.4f}, val FDEp: {:.4f}, val FDEb: {:.4f},val WSFDE: {:.4f}'\n",
    "        print(log.format(ADE_v, ADE_p, ADE_b, WSADE,FDE_v, FDE_p, FDE_b, WSFDE), flush=True)   \n",
    "    \n",
    "    return WSADE,WSFDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "Epoch: 1\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 2.9891\n",
      "Train Iter: 040, Train Loss: 1.7460\n",
      "Train Iter: 080, Train Loss: 1.3850\n",
      "Train Iter: 120, Train Loss: 1.2637\n",
      "val ADEv: 1.9133, val ADEp: 0.6939, val ADEb: 2.0662, val WSADE: 1.2397, val FDEv: 3.2484, val FDEp: 1.1793, val FDEb: 3.3169,val WSFDE: 2.0634\n",
      "epoch spend time: 31.7196\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 2\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 1.0143\n",
      "Train Iter: 040, Train Loss: 1.0160\n",
      "Train Iter: 080, Train Loss: 1.1911\n",
      "Train Iter: 120, Train Loss: 1.1380\n",
      "val ADEv: 1.7318, val ADEp: 0.6653, val ADEb: 1.8684, val WSADE: 1.1433, val FDEv: 2.9653, val FDEp: 1.1607, val FDEb: 3.0358,val WSFDE: 1.9341\n",
      "epoch spend time: 31.3398\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 3\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 1.0089\n",
      "Train Iter: 040, Train Loss: 1.0776\n",
      "Train Iter: 080, Train Loss: 0.9674\n",
      "Train Iter: 120, Train Loss: 0.8444\n",
      "val ADEv: 1.7116, val ADEp: 0.6766, val ADEb: 1.8873, val WSADE: 1.1500, val FDEv: 2.9932, val FDEp: 1.1822, val FDEb: 3.1821,val WSFDE: 1.9844\n",
      "epoch spend time: 31.2575\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 4\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 1.1066\n",
      "Train Iter: 040, Train Loss: 0.9360\n",
      "Train Iter: 080, Train Loss: 1.1281\n",
      "Train Iter: 120, Train Loss: 0.8156\n",
      "val ADEv: 1.6872, val ADEp: 0.6372, val ADEb: 1.8379, val WSADE: 1.1113, val FDEv: 2.9706, val FDEp: 1.1248, val FDEb: 3.1789,val WSFDE: 1.9458\n",
      "epoch spend time: 31.3235\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 5\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.9451\n",
      "Train Iter: 040, Train Loss: 0.8698\n",
      "Train Iter: 080, Train Loss: 0.7933\n",
      "Train Iter: 120, Train Loss: 0.9012\n",
      "val ADEv: 1.5721, val ADEp: 0.5647, val ADEb: 1.5718, val WSADE: 0.9878, val FDEv: 2.7789, val FDEp: 1.0131, val FDEb: 2.7049,val WSFDE: 1.7385\n",
      "epoch spend time: 31.1647\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 6\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 1.0532\n",
      "Train Iter: 040, Train Loss: 0.8186\n",
      "Train Iter: 080, Train Loss: 0.8282\n",
      "Train Iter: 120, Train Loss: 1.1181\n",
      "val ADEv: 1.4966, val ADEp: 0.5320, val ADEb: 1.5014, val WSADE: 0.9382, val FDEv: 2.6371, val FDEp: 0.9466, val FDEb: 2.5865,val WSFDE: 1.6455\n",
      "epoch spend time: 31.0320\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 7\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7566\n",
      "Train Iter: 040, Train Loss: 0.9510\n",
      "Train Iter: 080, Train Loss: 0.9005\n",
      "Train Iter: 120, Train Loss: 1.0338\n",
      "val ADEv: 1.4777, val ADEp: 0.5641, val ADEb: 1.5281, val WSADE: 0.9589, val FDEv: 2.6093, val FDEp: 1.0145, val FDEb: 2.6470,val WSFDE: 1.6926\n",
      "epoch spend time: 31.0329\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 8\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7950\n",
      "Train Iter: 040, Train Loss: 1.0659\n",
      "Train Iter: 080, Train Loss: 0.9573\n",
      "Train Iter: 120, Train Loss: 0.9958\n",
      "val ADEv: 1.4838, val ADEp: 0.5399, val ADEb: 1.5984, val WSADE: 0.9615, val FDEv: 2.6147, val FDEp: 0.9649, val FDEb: 2.7853,val WSFDE: 1.6953\n",
      "epoch spend time: 30.9476\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 9\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8876\n",
      "Train Iter: 040, Train Loss: 0.8861\n",
      "Train Iter: 080, Train Loss: 0.9287\n",
      "Train Iter: 120, Train Loss: 1.0170\n",
      "val ADEv: 1.4964, val ADEp: 0.5312, val ADEb: 1.5489, val WSADE: 0.9481, val FDEv: 2.6585, val FDEp: 0.9541, val FDEb: 2.6916,val WSFDE: 1.6772\n",
      "epoch spend time: 30.8559\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 10\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8100\n",
      "Train Iter: 040, Train Loss: 0.7108\n",
      "Train Iter: 080, Train Loss: 0.7487\n",
      "Train Iter: 120, Train Loss: 0.7440\n",
      "val ADEv: 1.4171, val ADEp: 0.5427, val ADEb: 1.5083, val WSADE: 0.9300, val FDEv: 2.4911, val FDEp: 0.9632, val FDEb: 2.5692,val WSFDE: 1.6221\n",
      "epoch spend time: 30.7743\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 11\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7265\n",
      "Train Iter: 040, Train Loss: 0.8785\n",
      "Train Iter: 080, Train Loss: 0.8442\n",
      "Train Iter: 120, Train Loss: 0.7461\n",
      "val ADEv: 1.3948, val ADEp: 0.5140, val ADEb: 1.4894, val WSADE: 0.9048, val FDEv: 2.4701, val FDEp: 0.9222, val FDEb: 2.5967,val WSFDE: 1.6002\n",
      "epoch spend time: 30.6788\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 12\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7233\n",
      "Train Iter: 040, Train Loss: 0.7951\n",
      "Train Iter: 080, Train Loss: 0.9515\n",
      "Train Iter: 120, Train Loss: 0.8440\n",
      "val ADEv: 1.3750, val ADEp: 0.5275, val ADEb: 1.4989, val WSADE: 0.9107, val FDEv: 2.4534, val FDEp: 0.9442, val FDEb: 2.6349,val WSFDE: 1.6180\n",
      "epoch spend time: 30.8571\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 13\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 1.0346\n",
      "Train Iter: 040, Train Loss: 0.8588\n",
      "Train Iter: 080, Train Loss: 1.0381\n",
      "Train Iter: 120, Train Loss: 0.7976\n",
      "val ADEv: 1.3904, val ADEp: 0.5244, val ADEb: 1.4478, val WSADE: 0.9007, val FDEv: 2.4725, val FDEp: 0.9348, val FDEb: 2.5339,val WSFDE: 1.5941\n",
      "epoch spend time: 30.9862\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 14\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.9901\n",
      "Train Iter: 040, Train Loss: 0.7230\n",
      "Train Iter: 080, Train Loss: 0.8236\n",
      "Train Iter: 120, Train Loss: 0.8456\n",
      "val ADEv: 1.3728, val ADEp: 0.5134, val ADEb: 1.4865, val WSADE: 0.8994, val FDEv: 2.4328, val FDEp: 0.9302, val FDEb: 2.6129,val WSFDE: 1.6009\n",
      "epoch spend time: 30.7967\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 15\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7596\n",
      "Train Iter: 040, Train Loss: 0.8883\n",
      "Train Iter: 080, Train Loss: 0.7512\n",
      "Train Iter: 120, Train Loss: 0.8958\n",
      "val ADEv: 1.4098, val ADEp: 0.5037, val ADEb: 1.4255, val WSADE: 0.8877, val FDEv: 2.4968, val FDEp: 0.9016, val FDEb: 2.4502,val WSFDE: 1.5613\n",
      "epoch spend time: 30.6968\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 16\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8091\n",
      "Train Iter: 040, Train Loss: 0.7389\n",
      "Train Iter: 080, Train Loss: 0.8627\n",
      "Train Iter: 120, Train Loss: 0.5950\n",
      "val ADEv: 1.3656, val ADEp: 0.5410, val ADEb: 1.3961, val WSADE: 0.8941, val FDEv: 2.4093, val FDEp: 0.9604, val FDEb: 2.3992,val WSFDE: 1.5667\n",
      "epoch spend time: 31.1197\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 17\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7689\n",
      "Train Iter: 040, Train Loss: 0.7578\n",
      "Train Iter: 080, Train Loss: 0.8167\n",
      "Train Iter: 120, Train Loss: 0.8178\n",
      "val ADEv: 1.3507, val ADEp: 0.5130, val ADEb: 1.4948, val WSADE: 0.8965, val FDEv: 2.4019, val FDEp: 0.9212, val FDEb: 2.5945,val WSFDE: 1.5854\n",
      "epoch spend time: 30.8952\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 18\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8702\n",
      "Train Iter: 040, Train Loss: 0.7810\n",
      "Train Iter: 080, Train Loss: 0.6575\n",
      "Train Iter: 120, Train Loss: 0.7777\n",
      "val ADEv: 1.3356, val ADEp: 0.5001, val ADEb: 1.3939, val WSADE: 0.8639, val FDEv: 2.3714, val FDEp: 0.8947, val FDEb: 2.4164,val WSFDE: 1.5248\n",
      "epoch spend time: 31.0349\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 19\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6980\n",
      "Train Iter: 040, Train Loss: 0.8114\n",
      "Train Iter: 080, Train Loss: 0.9606\n",
      "Train Iter: 120, Train Loss: 0.9998\n",
      "val ADEv: 1.3399, val ADEp: 0.4978, val ADEb: 1.3649, val WSADE: 0.8570, val FDEv: 2.3704, val FDEp: 0.8913, val FDEb: 2.3798,val WSFDE: 1.5146\n",
      "epoch spend time: 30.5000\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 20\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8608\n",
      "Train Iter: 040, Train Loss: 0.7736\n",
      "Train Iter: 080, Train Loss: 0.8076\n",
      "Train Iter: 120, Train Loss: 0.8645\n",
      "val ADEv: 1.3025, val ADEp: 0.4883, val ADEb: 1.3996, val WSADE: 0.8516, val FDEv: 2.3095, val FDEp: 0.8703, val FDEb: 2.4229,val WSFDE: 1.4997\n",
      "epoch spend time: 31.2340\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 21\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7301\n",
      "Train Iter: 040, Train Loss: 0.9451\n",
      "Train Iter: 080, Train Loss: 0.7386\n",
      "Train Iter: 120, Train Loss: 0.8731\n",
      "val ADEv: 1.3244, val ADEp: 0.4961, val ADEb: 1.4322, val WSADE: 0.8677, val FDEv: 2.3584, val FDEp: 0.8906, val FDEb: 2.5005,val WSFDE: 1.5383\n",
      "epoch spend time: 31.2549\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 22\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7433\n",
      "Train Iter: 040, Train Loss: 0.9085\n",
      "Train Iter: 080, Train Loss: 0.9324\n",
      "Train Iter: 120, Train Loss: 0.9062\n",
      "val ADEv: 1.3060, val ADEp: 0.4892, val ADEb: 1.3671, val WSADE: 0.8457, val FDEv: 2.3121, val FDEp: 0.8753, val FDEb: 2.3730,val WSFDE: 1.4921\n",
      "epoch spend time: 31.1744\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 23\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7380\n",
      "Train Iter: 040, Train Loss: 0.7661\n",
      "Train Iter: 080, Train Loss: 0.8480\n",
      "Train Iter: 120, Train Loss: 0.8455\n",
      "val ADEv: 1.3861, val ADEp: 0.5247, val ADEb: 1.4665, val WSADE: 0.9042, val FDEv: 2.4373, val FDEp: 0.9191, val FDEb: 2.4765,val WSFDE: 1.5654\n",
      "epoch spend time: 31.2488\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 24\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7524\n",
      "Train Iter: 040, Train Loss: 0.7992\n",
      "Train Iter: 080, Train Loss: 0.7875\n",
      "Train Iter: 120, Train Loss: 0.7511\n",
      "val ADEv: 1.3054, val ADEp: 0.5004, val ADEb: 1.4160, val WSADE: 0.8628, val FDEv: 2.3090, val FDEp: 0.8966, val FDEb: 2.4609,val WSFDE: 1.5232\n",
      "epoch spend time: 31.2485\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 25\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.9629\n",
      "Train Iter: 040, Train Loss: 0.7712\n",
      "Train Iter: 080, Train Loss: 0.7065\n",
      "Train Iter: 120, Train Loss: 0.7668\n",
      "val ADEv: 1.2820, val ADEp: 0.4963, val ADEb: 1.4146, val WSADE: 0.8555, val FDEv: 2.2734, val FDEp: 0.8869, val FDEb: 2.4582,val WSFDE: 1.5099\n",
      "epoch spend time: 31.2415\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 26\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7156\n",
      "Train Iter: 040, Train Loss: 0.6996\n",
      "Train Iter: 080, Train Loss: 0.8924\n",
      "Train Iter: 120, Train Loss: 0.8559\n",
      "val ADEv: 1.3022, val ADEp: 0.4855, val ADEb: 1.4025, val WSADE: 0.8506, val FDEv: 2.2982, val FDEp: 0.8646, val FDEb: 2.4407,val WSFDE: 1.4980\n",
      "epoch spend time: 31.2087\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 27\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7595\n",
      "Train Iter: 040, Train Loss: 0.9297\n",
      "Train Iter: 080, Train Loss: 0.7995\n",
      "Train Iter: 120, Train Loss: 0.7099\n",
      "val ADEv: 1.3789, val ADEp: 0.4991, val ADEb: 1.4752, val WSADE: 0.8898, val FDEv: 2.4697, val FDEp: 0.8903, val FDEb: 2.5791,val WSFDE: 1.5777\n",
      "epoch spend time: 31.2577\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 28\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7109\n",
      "Train Iter: 040, Train Loss: 0.8107\n",
      "Train Iter: 080, Train Loss: 0.6347\n",
      "Train Iter: 120, Train Loss: 0.7415\n",
      "val ADEv: 1.3013, val ADEp: 0.5134, val ADEb: 1.3904, val WSADE: 0.8639, val FDEv: 2.2999, val FDEp: 0.9067, val FDEb: 2.4213,val WSFDE: 1.5185\n",
      "epoch spend time: 30.9842\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 29\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6527\n",
      "Train Iter: 040, Train Loss: 0.7634\n",
      "Train Iter: 080, Train Loss: 0.7566\n",
      "Train Iter: 120, Train Loss: 0.8122\n",
      "val ADEv: 1.2891, val ADEp: 0.4893, val ADEb: 1.3406, val WSADE: 0.8366, val FDEv: 2.2764, val FDEp: 0.8672, val FDEb: 2.2982,val WSFDE: 1.4639\n",
      "epoch spend time: 30.9550\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 30\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7630\n",
      "Train Iter: 040, Train Loss: 0.8180\n",
      "Train Iter: 080, Train Loss: 0.7856\n",
      "Train Iter: 120, Train Loss: 0.7141\n",
      "val ADEv: 1.2996, val ADEp: 0.4865, val ADEb: 1.3615, val WSADE: 0.8416, val FDEv: 2.2858, val FDEp: 0.8662, val FDEb: 2.3269,val WSFDE: 1.4715\n",
      "epoch spend time: 31.1346\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 31\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7900\n",
      "Train Iter: 040, Train Loss: 0.5793\n",
      "Train Iter: 080, Train Loss: 0.6926\n",
      "Train Iter: 120, Train Loss: 0.7137\n",
      "val ADEv: 1.3063, val ADEp: 0.4986, val ADEb: 1.4092, val WSADE: 0.8605, val FDEv: 2.3112, val FDEp: 0.8960, val FDEb: 2.4331,val WSFDE: 1.5172\n",
      "epoch spend time: 30.9459\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 32\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.9569\n",
      "Train Iter: 040, Train Loss: 0.6872\n",
      "Train Iter: 080, Train Loss: 1.0322\n",
      "Train Iter: 120, Train Loss: 0.7589\n",
      "val ADEv: 1.2815, val ADEp: 0.4680, val ADEb: 1.4329, val WSADE: 0.8430, val FDEv: 2.2544, val FDEp: 0.8420, val FDEb: 2.4952,val WSFDE: 1.4882\n",
      "epoch spend time: 31.0755\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 33\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8904\n",
      "Train Iter: 040, Train Loss: 0.8432\n",
      "Train Iter: 080, Train Loss: 0.7754\n",
      "Train Iter: 120, Train Loss: 0.7106\n",
      "val ADEv: 1.2827, val ADEp: 0.4755, val ADEb: 1.3730, val WSADE: 0.8344, val FDEv: 2.2624, val FDEp: 0.8508, val FDEb: 2.3810,val WSFDE: 1.4697\n",
      "epoch spend time: 30.7570\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 34\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6821\n",
      "Train Iter: 040, Train Loss: 0.5720\n",
      "Train Iter: 080, Train Loss: 0.8685\n",
      "Train Iter: 120, Train Loss: 0.8331\n",
      "val ADEv: 1.2650, val ADEp: 0.4635, val ADEb: 1.3355, val WSADE: 0.8156, val FDEv: 2.2352, val FDEp: 0.8242, val FDEb: 2.2988,val WSFDE: 1.4308\n",
      "epoch spend time: 30.1599\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 35\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7840\n",
      "Train Iter: 040, Train Loss: 0.8219\n",
      "Train Iter: 080, Train Loss: 0.6834\n",
      "Train Iter: 120, Train Loss: 0.8592\n",
      "val ADEv: 1.2718, val ADEp: 0.4826, val ADEb: 1.4003, val WSADE: 0.8423, val FDEv: 2.2401, val FDEp: 0.8634, val FDEb: 2.4056,val WSFDE: 1.4780\n",
      "epoch spend time: 30.9970\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 36\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6271\n",
      "Train Iter: 040, Train Loss: 0.6068\n",
      "Train Iter: 080, Train Loss: 0.7125\n",
      "Train Iter: 120, Train Loss: 0.6751\n",
      "val ADEv: 1.2868, val ADEp: 0.4925, val ADEb: 1.3930, val WSADE: 0.8495, val FDEv: 2.2799, val FDEp: 0.8865, val FDEb: 2.4138,val WSFDE: 1.5012\n",
      "epoch spend time: 31.0822\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 37\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6618\n",
      "Train Iter: 040, Train Loss: 0.7093\n",
      "Train Iter: 080, Train Loss: 0.9791\n",
      "Train Iter: 120, Train Loss: 0.6891\n",
      "val ADEv: 1.2524, val ADEp: 0.4755, val ADEb: 1.3373, val WSADE: 0.8205, val FDEv: 2.2043, val FDEp: 0.8478, val FDEb: 2.2989,val WSFDE: 1.4383\n",
      "epoch spend time: 30.9182\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 38\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7122\n",
      "Train Iter: 040, Train Loss: 0.7945\n",
      "Train Iter: 080, Train Loss: 0.8326\n",
      "Train Iter: 120, Train Loss: 1.2067\n",
      "val ADEv: 1.2471, val ADEp: 0.4767, val ADEb: 1.3329, val WSADE: 0.8192, val FDEv: 2.1981, val FDEp: 0.8565, val FDEb: 2.3095,val WSFDE: 1.4445\n",
      "epoch spend time: 30.9590\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 39\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6289\n",
      "Train Iter: 040, Train Loss: 0.7633\n",
      "Train Iter: 080, Train Loss: 0.7580\n",
      "Train Iter: 120, Train Loss: 0.9497\n",
      "val ADEv: 1.2692, val ADEp: 0.4895, val ADEb: 1.3670, val WSADE: 0.8385, val FDEv: 2.2503, val FDEp: 0.8804, val FDEb: 2.3648,val WSFDE: 1.4809\n",
      "epoch spend time: 31.0657\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 40\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6883\n",
      "Train Iter: 040, Train Loss: 0.7370\n",
      "Train Iter: 080, Train Loss: 0.7083\n",
      "Train Iter: 120, Train Loss: 0.7494\n",
      "val ADEv: 1.2606, val ADEp: 0.4652, val ADEb: 1.3065, val WSADE: 0.8094, val FDEv: 2.2302, val FDEp: 0.8272, val FDEb: 2.2443,val WSFDE: 1.4196\n",
      "epoch spend time: 30.8384\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 41\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7137\n",
      "Train Iter: 040, Train Loss: 0.7030\n",
      "Train Iter: 080, Train Loss: 0.7791\n",
      "Train Iter: 120, Train Loss: 0.7445\n",
      "val ADEv: 1.2605, val ADEp: 0.4836, val ADEb: 1.3352, val WSADE: 0.8263, val FDEv: 2.2208, val FDEp: 0.8619, val FDEb: 2.3182,val WSFDE: 1.4541\n",
      "epoch spend time: 31.0458\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 42\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7055\n",
      "Train Iter: 040, Train Loss: 0.7678\n",
      "Train Iter: 080, Train Loss: 0.7278\n",
      "Train Iter: 120, Train Loss: 0.8228\n",
      "val ADEv: 1.2591, val ADEp: 0.4806, val ADEb: 1.3755, val WSADE: 0.8332, val FDEv: 2.2295, val FDEp: 0.8605, val FDEb: 2.3887,val WSFDE: 1.4705\n",
      "epoch spend time: 30.9285\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 43\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7725\n",
      "Train Iter: 040, Train Loss: 0.6705\n",
      "Train Iter: 080, Train Loss: 0.8255\n",
      "Train Iter: 120, Train Loss: 0.6052\n",
      "val ADEv: 1.2541, val ADEp: 0.4686, val ADEb: 1.3549, val WSADE: 0.8207, val FDEv: 2.2119, val FDEp: 0.8361, val FDEb: 2.3216,val WSFDE: 1.4381\n",
      "epoch spend time: 30.9626\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 44\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6799\n",
      "Train Iter: 040, Train Loss: 0.7141\n",
      "Train Iter: 080, Train Loss: 0.6572\n",
      "Train Iter: 120, Train Loss: 0.7819\n",
      "val ADEv: 1.2857, val ADEp: 0.4905, val ADEb: 1.3168, val WSADE: 0.8313, val FDEv: 2.2842, val FDEp: 0.8721, val FDEb: 2.2696,val WSFDE: 1.4620\n",
      "epoch spend time: 30.8311\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 45\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8102\n",
      "Train Iter: 040, Train Loss: 0.6329\n",
      "Train Iter: 080, Train Loss: 0.5997\n",
      "Train Iter: 120, Train Loss: 0.6689\n",
      "val ADEv: 1.2662, val ADEp: 0.4622, val ADEb: 1.3577, val WSADE: 0.8200, val FDEv: 2.2398, val FDEp: 0.8318, val FDEb: 2.3518,val WSFDE: 1.4478\n",
      "epoch spend time: 30.8133\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 46\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8310\n",
      "Train Iter: 040, Train Loss: 0.8687\n",
      "Train Iter: 080, Train Loss: 0.6976\n",
      "Train Iter: 120, Train Loss: 0.7920\n",
      "val ADEv: 1.2149, val ADEp: 0.4594, val ADEb: 1.3527, val WSADE: 0.8070, val FDEv: 2.1344, val FDEp: 0.8193, val FDEb: 2.3254,val WSFDE: 1.4137\n",
      "epoch spend time: 30.6427\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 47\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6671\n",
      "Train Iter: 040, Train Loss: 0.5684\n",
      "Train Iter: 080, Train Loss: 0.6362\n",
      "Train Iter: 120, Train Loss: 0.5975\n",
      "val ADEv: 1.2120, val ADEp: 0.4687, val ADEb: 1.3246, val WSADE: 0.8057, val FDEv: 2.1282, val FDEp: 0.8386, val FDEb: 2.2737,val WSFDE: 1.4122\n",
      "epoch spend time: 31.2940\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 48\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7535\n",
      "Train Iter: 040, Train Loss: 0.6508\n",
      "Train Iter: 080, Train Loss: 0.9186\n",
      "Train Iter: 120, Train Loss: 0.7624\n",
      "val ADEv: 1.2538, val ADEp: 0.4491, val ADEb: 1.3788, val WSADE: 0.8146, val FDEv: 2.2132, val FDEp: 0.8019, val FDEb: 2.3654,val WSFDE: 1.4281\n",
      "epoch spend time: 31.1382\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 49\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6398\n",
      "Train Iter: 040, Train Loss: 0.9485\n",
      "Train Iter: 080, Train Loss: 0.7838\n",
      "Train Iter: 120, Train Loss: 0.7619\n",
      "val ADEv: 1.2347, val ADEp: 0.4689, val ADEb: 1.3461, val WSADE: 0.8150, val FDEv: 2.1689, val FDEp: 0.8383, val FDEb: 2.3321,val WSFDE: 1.4331\n",
      "epoch spend time: 31.0056\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 50\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 1.0371\n",
      "Train Iter: 040, Train Loss: 0.6705\n",
      "Train Iter: 080, Train Loss: 0.5767\n",
      "Train Iter: 120, Train Loss: 0.8734\n",
      "val ADEv: 1.2359, val ADEp: 0.4577, val ADEb: 1.3258, val WSADE: 0.8043, val FDEv: 2.1737, val FDEp: 0.8143, val FDEb: 2.2812,val WSFDE: 1.4089\n",
      "epoch spend time: 30.9540\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 51\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.9496\n",
      "Train Iter: 040, Train Loss: 0.7364\n",
      "Train Iter: 080, Train Loss: 0.8370\n",
      "Train Iter: 120, Train Loss: 0.6361\n",
      "val ADEv: 1.2284, val ADEp: 0.4599, val ADEb: 1.3209, val WSADE: 0.8030, val FDEv: 2.1538, val FDEp: 0.8126, val FDEb: 2.2404,val WSFDE: 1.3950\n",
      "epoch spend time: 31.0306\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 52\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7301\n",
      "Train Iter: 040, Train Loss: 0.8231\n",
      "Train Iter: 080, Train Loss: 0.8774\n",
      "Train Iter: 120, Train Loss: 0.6993\n",
      "val ADEv: 1.2281, val ADEp: 0.4750, val ADEb: 1.3478, val WSADE: 0.8176, val FDEv: 2.1611, val FDEp: 0.8468, val FDEb: 2.3294,val WSFDE: 1.4358\n",
      "epoch spend time: 31.2514\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 53\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8814\n",
      "Train Iter: 040, Train Loss: 0.8962\n",
      "Train Iter: 080, Train Loss: 0.6159\n",
      "Train Iter: 120, Train Loss: 0.5855\n",
      "val ADEv: 1.2239, val ADEp: 0.4634, val ADEb: 1.3283, val WSADE: 0.8057, val FDEv: 2.1558, val FDEp: 0.8248, val FDEb: 2.2768,val WSFDE: 1.4104\n",
      "epoch spend time: 30.9406\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 54\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 1.0233\n",
      "Train Iter: 040, Train Loss: 0.9512\n",
      "Train Iter: 080, Train Loss: 0.5084\n",
      "Train Iter: 120, Train Loss: 0.8556\n",
      "val ADEv: 1.2075, val ADEp: 0.4636, val ADEb: 1.3004, val WSADE: 0.7965, val FDEv: 2.1156, val FDEp: 0.8219, val FDEb: 2.2370,val WSFDE: 1.3920\n",
      "epoch spend time: 31.1536\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 55\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6928\n",
      "Train Iter: 040, Train Loss: 0.7250\n",
      "Train Iter: 080, Train Loss: 0.8717\n",
      "Train Iter: 120, Train Loss: 0.7803\n",
      "val ADEv: 1.2159, val ADEp: 0.4647, val ADEb: 1.3101, val WSADE: 0.8009, val FDEv: 2.1455, val FDEp: 0.8327, val FDEb: 2.2704,val WSFDE: 1.4116\n",
      "epoch spend time: 31.1791\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 56\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8426\n",
      "Train Iter: 040, Train Loss: 0.8825\n",
      "Train Iter: 080, Train Loss: 0.8626\n",
      "Train Iter: 120, Train Loss: 0.7507\n",
      "val ADEv: 1.2140, val ADEp: 0.4613, val ADEb: 1.3051, val WSADE: 0.7975, val FDEv: 2.1270, val FDEp: 0.8221, val FDEb: 2.2584,val WSFDE: 1.3990\n",
      "epoch spend time: 31.0235\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 57\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7607\n",
      "Train Iter: 040, Train Loss: 0.6458\n",
      "Train Iter: 080, Train Loss: 0.6583\n",
      "Train Iter: 120, Train Loss: 1.1877\n",
      "val ADEv: 1.2072, val ADEp: 0.4578, val ADEb: 1.3088, val WSADE: 0.7949, val FDEv: 2.1222, val FDEp: 0.8190, val FDEb: 2.2707,val WSFDE: 1.3990\n",
      "epoch spend time: 31.2687\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 58\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7435\n",
      "Train Iter: 040, Train Loss: 0.8592\n",
      "Train Iter: 080, Train Loss: 0.6833\n",
      "Train Iter: 120, Train Loss: 0.8853\n",
      "val ADEv: 1.2175, val ADEp: 0.4691, val ADEb: 1.3367, val WSADE: 0.8096, val FDEv: 2.1331, val FDEp: 0.8395, val FDEb: 2.3158,val WSFDE: 1.4230\n",
      "epoch spend time: 30.5918\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 59\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8255\n",
      "Train Iter: 040, Train Loss: 0.8604\n",
      "Train Iter: 080, Train Loss: 0.6622\n",
      "Train Iter: 120, Train Loss: 0.6914\n",
      "val ADEv: 1.2433, val ADEp: 0.4798, val ADEb: 1.3735, val WSADE: 0.8291, val FDEv: 2.1944, val FDEp: 0.8533, val FDEb: 2.3984,val WSFDE: 1.4614\n",
      "epoch spend time: 31.0425\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 60\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8481\n",
      "Train Iter: 040, Train Loss: 0.7767\n",
      "Train Iter: 080, Train Loss: 0.7385\n",
      "Train Iter: 120, Train Loss: 1.0817\n",
      "val ADEv: 1.1988, val ADEp: 0.4618, val ADEb: 1.3216, val WSADE: 0.7983, val FDEv: 2.1047, val FDEp: 0.8254, val FDEb: 2.2840,val WSFDE: 1.4022\n",
      "epoch spend time: 31.2686\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 61\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7875\n",
      "Train Iter: 040, Train Loss: 0.6172\n",
      "Train Iter: 080, Train Loss: 0.7577\n",
      "Train Iter: 120, Train Loss: 0.7587\n",
      "val ADEv: 1.2297, val ADEp: 0.4541, val ADEb: 1.3551, val WSADE: 0.8074, val FDEv: 2.1667, val FDEp: 0.8112, val FDEb: 2.3550,val WSFDE: 1.4219\n",
      "epoch spend time: 31.1423\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 62\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6690\n",
      "Train Iter: 040, Train Loss: 0.7929\n",
      "Train Iter: 080, Train Loss: 0.6116\n",
      "Train Iter: 120, Train Loss: 0.6944\n",
      "val ADEv: 1.2493, val ADEp: 0.4806, val ADEb: 1.4062, val WSADE: 0.8380, val FDEv: 2.2004, val FDEp: 0.8602, val FDEb: 2.4596,val WSFDE: 1.4801\n",
      "epoch spend time: 30.9196\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 63\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6074\n",
      "Train Iter: 040, Train Loss: 0.7512\n",
      "Train Iter: 080, Train Loss: 0.6942\n",
      "Train Iter: 120, Train Loss: 0.6721\n",
      "val ADEv: 1.2265, val ADEp: 0.4820, val ADEb: 1.3747, val WSADE: 0.8273, val FDEv: 2.1578, val FDEp: 0.8671, val FDEb: 2.3776,val WSFDE: 1.4576\n",
      "epoch spend time: 31.0391\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 64\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7232\n",
      "Train Iter: 040, Train Loss: 0.6370\n",
      "Train Iter: 080, Train Loss: 0.6602\n",
      "Train Iter: 120, Train Loss: 0.7910\n",
      "val ADEv: 1.2081, val ADEp: 0.4593, val ADEb: 1.3268, val WSADE: 0.7999, val FDEv: 2.1239, val FDEp: 0.8241, val FDEb: 2.2930,val WSFDE: 1.4072\n",
      "epoch spend time: 31.2259\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 65\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7214\n",
      "Train Iter: 040, Train Loss: 0.8067\n",
      "Train Iter: 080, Train Loss: 0.6304\n",
      "Train Iter: 120, Train Loss: 0.6525\n",
      "val ADEv: 1.2159, val ADEp: 0.4612, val ADEb: 1.3376, val WSADE: 0.8049, val FDEv: 2.1453, val FDEp: 0.8280, val FDEb: 2.3415,val WSFDE: 1.4244\n",
      "epoch spend time: 31.3793\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 66\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7696\n",
      "Train Iter: 040, Train Loss: 0.7098\n",
      "Train Iter: 080, Train Loss: 0.7253\n",
      "Train Iter: 120, Train Loss: 0.7626\n",
      "val ADEv: 1.2196, val ADEp: 0.4567, val ADEb: 1.3069, val WSADE: 0.7963, val FDEv: 2.1372, val FDEp: 0.8142, val FDEb: 2.2646,val WSFDE: 1.3979\n",
      "epoch spend time: 31.3281\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 67\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7571\n",
      "Train Iter: 040, Train Loss: 0.7176\n",
      "Train Iter: 080, Train Loss: 0.5528\n",
      "Train Iter: 120, Train Loss: 1.0024\n",
      "val ADEv: 1.2027, val ADEp: 0.4556, val ADEb: 1.2858, val WSADE: 0.7877, val FDEv: 2.1062, val FDEp: 0.8108, val FDEb: 2.2018,val WSFDE: 1.3759\n",
      "epoch spend time: 31.3160\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 68\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.9944\n",
      "Train Iter: 040, Train Loss: 0.7306\n",
      "Train Iter: 080, Train Loss: 1.0005\n",
      "Train Iter: 120, Train Loss: 0.8083\n",
      "val ADEv: 1.2453, val ADEp: 0.4690, val ADEb: 1.3887, val WSADE: 0.8266, val FDEv: 2.2016, val FDEp: 0.8446, val FDEb: 2.4345,val WSFDE: 1.4658\n",
      "epoch spend time: 31.0482\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 69\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6589\n",
      "Train Iter: 040, Train Loss: 0.8320\n",
      "Train Iter: 080, Train Loss: 0.7353\n",
      "Train Iter: 120, Train Loss: 0.7421\n",
      "val ADEv: 1.1793, val ADEp: 0.4667, val ADEb: 1.2727, val WSADE: 0.7866, val FDEv: 2.0583, val FDEp: 0.8350, val FDEb: 2.1924,val WSFDE: 1.3783\n",
      "epoch spend time: 31.1465\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 70\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.9197\n",
      "Train Iter: 040, Train Loss: 0.5504\n",
      "Train Iter: 080, Train Loss: 0.6818\n",
      "Train Iter: 120, Train Loss: 0.7679\n",
      "val ADEv: 1.2230, val ADEp: 0.4484, val ADEb: 1.2750, val WSADE: 0.7852, val FDEv: 2.1354, val FDEp: 0.7975, val FDEb: 2.1910,val WSFDE: 1.3716\n",
      "epoch spend time: 31.2000\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 71\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7677\n",
      "Train Iter: 040, Train Loss: 0.6606\n",
      "Train Iter: 080, Train Loss: 0.9628\n",
      "Train Iter: 120, Train Loss: 0.8050\n",
      "val ADEv: 1.2056, val ADEp: 0.4659, val ADEb: 1.3272, val WSADE: 0.8033, val FDEv: 2.1145, val FDEp: 0.8327, val FDEb: 2.2873,val WSFDE: 1.4091\n",
      "epoch spend time: 30.2556\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 72\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7038\n",
      "Train Iter: 040, Train Loss: 0.7988\n",
      "Train Iter: 080, Train Loss: 0.8589\n",
      "Train Iter: 120, Train Loss: 0.6003\n",
      "val ADEv: 1.2384, val ADEp: 0.4814, val ADEb: 1.3597, val WSADE: 0.8260, val FDEv: 2.1809, val FDEp: 0.8618, val FDEb: 2.3542,val WSFDE: 1.4540\n",
      "epoch spend time: 30.2038\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 73\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8323\n",
      "Train Iter: 040, Train Loss: 0.7385\n",
      "Train Iter: 080, Train Loss: 0.8643\n",
      "Train Iter: 120, Train Loss: 0.7153\n",
      "val ADEv: 1.2033, val ADEp: 0.4637, val ADEb: 1.3176, val WSADE: 0.7995, val FDEv: 2.1034, val FDEp: 0.8280, val FDEb: 2.2716,val WSFDE: 1.4007\n",
      "epoch spend time: 30.9214\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 74\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7298\n",
      "Train Iter: 040, Train Loss: 0.7144\n",
      "Train Iter: 080, Train Loss: 0.8121\n",
      "Train Iter: 120, Train Loss: 0.8257\n",
      "val ADEv: 1.1858, val ADEp: 0.4543, val ADEb: 1.3163, val WSADE: 0.7902, val FDEv: 2.0743, val FDEp: 0.8091, val FDEb: 2.2588,val WSFDE: 1.3811\n",
      "epoch spend time: 30.6982\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 75\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.5982\n",
      "Train Iter: 040, Train Loss: 0.7540\n",
      "Train Iter: 080, Train Loss: 0.7658\n",
      "Train Iter: 120, Train Loss: 0.8928\n",
      "val ADEv: 1.2030, val ADEp: 0.4627, val ADEb: 1.3245, val WSADE: 0.8004, val FDEv: 2.1087, val FDEp: 0.8252, val FDEb: 2.2714,val WSFDE: 1.4001\n",
      "epoch spend time: 30.7401\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 76\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7447\n",
      "Train Iter: 040, Train Loss: 0.6172\n",
      "Train Iter: 080, Train Loss: 0.8532\n",
      "Train Iter: 120, Train Loss: 0.8485\n",
      "val ADEv: 1.1958, val ADEp: 0.4544, val ADEb: 1.3857, val WSADE: 0.8076, val FDEv: 2.1078, val FDEp: 0.8130, val FDEb: 2.3958,val WSFDE: 1.4202\n",
      "epoch spend time: 30.7757\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 77\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6265\n",
      "Train Iter: 040, Train Loss: 0.6175\n",
      "Train Iter: 080, Train Loss: 0.5666\n",
      "Train Iter: 120, Train Loss: 0.6084\n",
      "val ADEv: 1.1902, val ADEp: 0.4429, val ADEb: 1.2977, val WSADE: 0.7805, val FDEv: 2.0719, val FDEp: 0.7837, val FDEb: 2.1823,val WSFDE: 1.3490\n",
      "epoch spend time: 30.0514\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 78\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6579\n",
      "Train Iter: 040, Train Loss: 0.6078\n",
      "Train Iter: 080, Train Loss: 0.5544\n",
      "Train Iter: 120, Train Loss: 0.7734\n",
      "val ADEv: 1.2033, val ADEp: 0.4583, val ADEb: 1.3379, val WSADE: 0.8008, val FDEv: 2.1137, val FDEp: 0.8221, val FDEb: 2.2987,val WSFDE: 1.4053\n",
      "epoch spend time: 30.7269\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 79\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6479\n",
      "Train Iter: 040, Train Loss: 0.7946\n",
      "Train Iter: 080, Train Loss: 0.6041\n",
      "Train Iter: 120, Train Loss: 0.6966\n",
      "val ADEv: 1.1957, val ADEp: 0.4469, val ADEb: 1.2824, val WSADE: 0.7805, val FDEv: 2.0830, val FDEp: 0.7880, val FDEb: 2.1929,val WSFDE: 1.3561\n",
      "epoch spend time: 30.7767\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 80\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6436\n",
      "Train Iter: 040, Train Loss: 0.7348\n",
      "Train Iter: 080, Train Loss: 0.6675\n",
      "Train Iter: 120, Train Loss: 0.6799\n",
      "val ADEv: 1.1855, val ADEp: 0.4691, val ADEb: 1.3116, val WSADE: 0.7977, val FDEv: 2.0697, val FDEp: 0.8340, val FDEb: 2.2614,val WSFDE: 1.3952\n",
      "epoch spend time: 30.7171\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 81\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6730\n",
      "Train Iter: 040, Train Loss: 0.8822\n",
      "Train Iter: 080, Train Loss: 0.4558\n",
      "Train Iter: 120, Train Loss: 0.8407\n",
      "val ADEv: 1.1896, val ADEp: 0.4605, val ADEb: 1.3022, val WSADE: 0.7915, val FDEv: 2.0844, val FDEp: 0.8220, val FDEb: 2.2319,val WSFDE: 1.3847\n",
      "epoch spend time: 30.8253\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 82\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6080\n",
      "Train Iter: 040, Train Loss: 0.5180\n",
      "Train Iter: 080, Train Loss: 0.6858\n",
      "Train Iter: 120, Train Loss: 0.6278\n",
      "val ADEv: 1.2099, val ADEp: 0.4678, val ADEb: 1.2923, val WSADE: 0.7976, val FDEv: 2.1204, val FDEp: 0.8289, val FDEb: 2.2122,val WSFDE: 1.3915\n",
      "epoch spend time: 30.9193\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 83\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7710\n",
      "Train Iter: 040, Train Loss: 0.6850\n",
      "Train Iter: 080, Train Loss: 0.6929\n",
      "Train Iter: 120, Train Loss: 0.5653\n",
      "val ADEv: 1.2071, val ADEp: 0.4524, val ADEb: 1.3109, val WSADE: 0.7922, val FDEv: 2.1041, val FDEp: 0.8075, val FDEb: 2.2418,val WSFDE: 1.3824\n",
      "epoch spend time: 30.7556\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 84\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7881\n",
      "Train Iter: 040, Train Loss: 0.6930\n",
      "Train Iter: 080, Train Loss: 0.6228\n",
      "Train Iter: 120, Train Loss: 0.7389\n",
      "val ADEv: 1.1829, val ADEp: 0.4578, val ADEb: 1.3061, val WSADE: 0.7895, val FDEv: 2.0727, val FDEp: 0.8209, val FDEb: 2.2623,val WSFDE: 1.3884\n",
      "epoch spend time: 31.1069\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 85\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7223\n",
      "Train Iter: 040, Train Loss: 0.7122\n",
      "Train Iter: 080, Train Loss: 0.6842\n",
      "Train Iter: 120, Train Loss: 0.7936\n",
      "val ADEv: 1.1780, val ADEp: 0.4557, val ADEb: 1.2787, val WSADE: 0.7812, val FDEv: 2.0574, val FDEp: 0.8086, val FDEb: 2.1997,val WSFDE: 1.3644\n",
      "epoch spend time: 31.0753\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 86\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7898\n",
      "Train Iter: 040, Train Loss: 0.7492\n",
      "Train Iter: 080, Train Loss: 0.6129\n",
      "Train Iter: 120, Train Loss: 0.6935\n",
      "val ADEv: 1.1756, val ADEp: 0.4569, val ADEb: 1.2467, val WSADE: 0.7744, val FDEv: 2.0425, val FDEp: 0.8122, val FDEb: 2.1258,val WSFDE: 1.3473\n",
      "epoch spend time: 31.1299\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 87\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.9651\n",
      "Train Iter: 040, Train Loss: 0.6820\n",
      "Train Iter: 080, Train Loss: 0.7634\n",
      "Train Iter: 120, Train Loss: 0.8434\n",
      "val ADEv: 1.2161, val ADEp: 0.4521, val ADEb: 1.3148, val WSADE: 0.7947, val FDEv: 2.1252, val FDEp: 0.8054, val FDEb: 2.2726,val WSFDE: 1.3922\n",
      "epoch spend time: 31.1400\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 88\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8547\n",
      "Train Iter: 040, Train Loss: 0.5876\n",
      "Train Iter: 080, Train Loss: 0.7435\n",
      "Train Iter: 120, Train Loss: 0.7075\n",
      "val ADEv: 1.1843, val ADEp: 0.4799, val ADEb: 1.2843, val WSADE: 0.7978, val FDEv: 2.0720, val FDEp: 0.8602, val FDEb: 2.2161,val WSFDE: 1.4009\n",
      "epoch spend time: 31.2112\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 89\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8536\n",
      "Train Iter: 040, Train Loss: 0.7282\n",
      "Train Iter: 080, Train Loss: 0.6061\n",
      "Train Iter: 120, Train Loss: 0.6613\n",
      "val ADEv: 1.2019, val ADEp: 0.4648, val ADEb: 1.3114, val WSADE: 0.7985, val FDEv: 2.1028, val FDEp: 0.8312, val FDEb: 2.2814,val WSFDE: 1.4046\n",
      "epoch spend time: 31.1480\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 90\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6872\n",
      "Train Iter: 040, Train Loss: 0.6291\n",
      "Train Iter: 080, Train Loss: 0.7595\n",
      "Train Iter: 120, Train Loss: 0.6756\n",
      "val ADEv: 1.2296, val ADEp: 0.4615, val ADEb: 1.3364, val WSADE: 0.8076, val FDEv: 2.1686, val FDEp: 0.8276, val FDEb: 2.2977,val WSFDE: 1.4192\n",
      "epoch spend time: 30.5177\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 91\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7467\n",
      "Train Iter: 040, Train Loss: 0.5608\n",
      "Train Iter: 080, Train Loss: 0.5752\n",
      "Train Iter: 120, Train Loss: 0.6649\n",
      "val ADEv: 1.1823, val ADEp: 0.4587, val ADEb: 1.3206, val WSADE: 0.7931, val FDEv: 2.0689, val FDEp: 0.8200, val FDEb: 2.2788,val WSFDE: 1.3907\n",
      "epoch spend time: 31.3356\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 92\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7024\n",
      "Train Iter: 040, Train Loss: 0.8175\n",
      "Train Iter: 080, Train Loss: 0.7230\n",
      "Train Iter: 120, Train Loss: 0.6772\n",
      "val ADEv: 1.1772, val ADEp: 0.4471, val ADEb: 1.3077, val WSADE: 0.7825, val FDEv: 2.0576, val FDEp: 0.7971, val FDEb: 2.2543,val WSFDE: 1.3698\n",
      "epoch spend time: 30.6031\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 93\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.9192\n",
      "Train Iter: 040, Train Loss: 0.6384\n",
      "Train Iter: 080, Train Loss: 0.8222\n",
      "Train Iter: 120, Train Loss: 0.8957\n",
      "val ADEv: 1.2237, val ADEp: 0.4743, val ADEb: 1.3336, val WSADE: 0.8132, val FDEv: 2.1393, val FDEp: 0.8415, val FDEb: 2.3239,val WSFDE: 1.4272\n",
      "epoch spend time: 31.1275\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 94\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6035\n",
      "Train Iter: 040, Train Loss: 0.6012\n",
      "Train Iter: 080, Train Loss: 0.6055\n",
      "Train Iter: 120, Train Loss: 0.5977\n",
      "val ADEv: 1.1730, val ADEp: 0.4493, val ADEb: 1.3010, val WSADE: 0.7814, val FDEv: 2.0485, val FDEp: 0.8027, val FDEb: 2.2445,val WSFDE: 1.3690\n",
      "epoch spend time: 30.9735\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 95\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7093\n",
      "Train Iter: 040, Train Loss: 0.5914\n",
      "Train Iter: 080, Train Loss: 0.6141\n",
      "Train Iter: 120, Train Loss: 0.6990\n",
      "val ADEv: 1.2018, val ADEp: 0.4546, val ADEb: 1.3424, val WSADE: 0.7994, val FDEv: 2.1100, val FDEp: 0.8129, val FDEb: 2.3009,val WSFDE: 1.3997\n",
      "epoch spend time: 31.3160\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 96\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.5930\n",
      "Train Iter: 040, Train Loss: 0.6556\n",
      "Train Iter: 080, Train Loss: 0.5848\n",
      "Train Iter: 120, Train Loss: 0.5945\n",
      "val ADEv: 1.1777, val ADEp: 0.4605, val ADEb: 1.2899, val WSADE: 0.7864, val FDEv: 2.0570, val FDEp: 0.8195, val FDEb: 2.2303,val WSFDE: 1.3774\n",
      "epoch spend time: 31.3016\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 97\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8394\n",
      "Train Iter: 040, Train Loss: 0.5194\n",
      "Train Iter: 080, Train Loss: 0.5841\n",
      "Train Iter: 120, Train Loss: 0.5902\n",
      "val ADEv: 1.1655, val ADEp: 0.4504, val ADEb: 1.2941, val WSADE: 0.7790, val FDEv: 2.0352, val FDEp: 0.8019, val FDEb: 2.2377,val WSFDE: 1.3645\n",
      "epoch spend time: 31.3247\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 98\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.8144\n",
      "Train Iter: 040, Train Loss: 0.6824\n",
      "Train Iter: 080, Train Loss: 0.8746\n",
      "Train Iter: 120, Train Loss: 0.6285\n",
      "val ADEv: 1.1848, val ADEp: 0.4478, val ADEb: 1.3031, val WSADE: 0.7834, val FDEv: 2.0779, val FDEp: 0.7994, val FDEb: 2.2482,val WSFDE: 1.3738\n",
      "epoch spend time: 31.1743\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 99\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7216\n",
      "Train Iter: 040, Train Loss: 0.6506\n",
      "Train Iter: 080, Train Loss: 0.8045\n",
      "Train Iter: 120, Train Loss: 0.6418\n",
      "val ADEv: 1.1812, val ADEp: 0.4578, val ADEb: 1.2932, val WSADE: 0.7862, val FDEv: 2.0596, val FDEp: 0.8140, val FDEb: 2.2277,val WSFDE: 1.3741\n",
      "epoch spend time: 31.3396\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 100\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7096\n",
      "Train Iter: 040, Train Loss: 0.7859\n",
      "Train Iter: 080, Train Loss: 0.7708\n",
      "Train Iter: 120, Train Loss: 0.6815\n",
      "val ADEv: 1.1831, val ADEp: 0.4531, val ADEb: 1.2710, val WSADE: 0.7790, val FDEv: 2.0546, val FDEp: 0.8016, val FDEb: 2.1815,val WSFDE: 1.3558\n",
      "epoch spend time: 31.2469\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 101\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.5981\n",
      "Train Iter: 040, Train Loss: 0.8072\n",
      "Train Iter: 080, Train Loss: 0.6179\n",
      "Train Iter: 120, Train Loss: 0.5625\n",
      "val ADEv: 1.1878, val ADEp: 0.4616, val ADEb: 1.2974, val WSADE: 0.7907, val FDEv: 2.0751, val FDEp: 0.8239, val FDEb: 2.2339,val WSFDE: 1.3843\n",
      "epoch spend time: 31.3378\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 102\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.6057\n",
      "Train Iter: 040, Train Loss: 0.5991\n",
      "Train Iter: 080, Train Loss: 0.6318\n",
      "Train Iter: 120, Train Loss: 0.5786\n",
      "val ADEv: 1.1699, val ADEp: 0.4619, val ADEb: 1.2907, val WSADE: 0.7859, val FDEv: 2.0403, val FDEp: 0.8212, val FDEb: 2.2049,val WSFDE: 1.3695\n",
      "epoch spend time: 31.1250\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 103\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.5521\n",
      "Train Iter: 040, Train Loss: 0.7053\n",
      "Train Iter: 080, Train Loss: 0.5565\n",
      "Train Iter: 120, Train Loss: 0.7391\n",
      "val ADEv: 1.1576, val ADEp: 0.4567, val ADEb: 1.2869, val WSADE: 0.7795, val FDEv: 2.0145, val FDEp: 0.8149, val FDEb: 2.2083,val WSFDE: 1.3614\n",
      "epoch spend time: 31.0480\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 104\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7018\n",
      "Train Iter: 040, Train Loss: 0.6538\n",
      "Train Iter: 080, Train Loss: 0.5617\n",
      "Train Iter: 120, Train Loss: 0.6178\n",
      "val ADEv: 1.1692, val ADEp: 0.4535, val ADEb: 1.2684, val WSADE: 0.7759, val FDEv: 2.0365, val FDEp: 0.8085, val FDEb: 2.1784,val WSFDE: 1.3555\n",
      "epoch spend time: 30.8711\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 105\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7793\n",
      "Train Iter: 040, Train Loss: 0.6326\n",
      "Train Iter: 080, Train Loss: 0.7272\n",
      "Train Iter: 120, Train Loss: 0.7768\n",
      "val ADEv: 1.2145, val ADEp: 0.4860, val ADEb: 1.4144, val WSADE: 0.8359, val FDEv: 2.1277, val FDEp: 0.8770, val FDEb: 2.4468,val WSFDE: 1.4725\n",
      "epoch spend time: 31.2368\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 106\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7266\n",
      "Train Iter: 040, Train Loss: 0.9548\n",
      "Train Iter: 080, Train Loss: 0.6843\n",
      "Train Iter: 120, Train Loss: 0.7799\n",
      "val ADEv: 1.1773, val ADEp: 0.4495, val ADEb: 1.3171, val WSADE: 0.7859, val FDEv: 2.0639, val FDEp: 0.8032, val FDEb: 2.2770,val WSFDE: 1.3796\n",
      "epoch spend time: 31.3290\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 107\n",
      "Learning Rate: 0.001\n",
      "Train Iter: 000, Train Loss: 0.7427\n",
      "Train Iter: 040, Train Loss: 0.7440\n",
      "Train Iter: 080, Train Loss: 0.8322\n",
      "Train Iter: 120, Train Loss: 0.7179\n",
      "val ADEv: 1.1513, val ADEp: 0.4480, val ADEb: 1.2690, val WSADE: 0.7693, val FDEv: 2.0036, val FDEp: 0.8027, val FDEb: 2.1740,val WSFDE: 1.3445\n",
      "epoch spend time: 31.3725\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 108\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.9809\n",
      "Train Iter: 040, Train Loss: 0.7845\n",
      "Train Iter: 080, Train Loss: 0.6881\n",
      "Train Iter: 120, Train Loss: 0.6935\n",
      "val ADEv: 1.1467, val ADEp: 0.4455, val ADEb: 1.2628, val WSADE: 0.7655, val FDEv: 1.9945, val FDEp: 0.7919, val FDEb: 2.1712,val WSFDE: 1.3358\n",
      "epoch spend time: 31.1483\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 109\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6533\n",
      "Train Iter: 040, Train Loss: 0.6861\n",
      "Train Iter: 080, Train Loss: 0.7174\n",
      "Train Iter: 120, Train Loss: 0.7400\n",
      "val ADEv: 1.1499, val ADEp: 0.4435, val ADEb: 1.2522, val WSADE: 0.7627, val FDEv: 2.0064, val FDEp: 0.7876, val FDEb: 2.1464,val WSFDE: 1.3303\n",
      "epoch spend time: 31.1086\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 110\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.7814\n",
      "Train Iter: 040, Train Loss: 0.6436\n",
      "Train Iter: 080, Train Loss: 0.7068\n",
      "Train Iter: 120, Train Loss: 0.7946\n",
      "val ADEv: 1.1565, val ADEp: 0.4577, val ADEb: 1.2986, val WSADE: 0.7824, val FDEv: 2.0184, val FDEp: 0.8196, val FDEb: 2.2462,val WSFDE: 1.3732\n",
      "epoch spend time: 31.2807\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 111\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.8237\n",
      "Train Iter: 040, Train Loss: 0.6298\n",
      "Train Iter: 080, Train Loss: 0.7517\n",
      "Train Iter: 120, Train Loss: 0.6731\n",
      "val ADEv: 1.1364, val ADEp: 0.4457, val ADEb: 1.2643, val WSADE: 0.7639, val FDEv: 1.9755, val FDEp: 0.7938, val FDEb: 2.1773,val WSFDE: 1.3345\n",
      "epoch spend time: 31.2253\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 112\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6678\n",
      "Train Iter: 040, Train Loss: 0.7893\n",
      "Train Iter: 080, Train Loss: 0.8120\n",
      "Train Iter: 120, Train Loss: 0.5093\n",
      "val ADEv: 1.1312, val ADEp: 0.4425, val ADEb: 1.2456, val WSADE: 0.7569, val FDEv: 1.9688, val FDEp: 0.7828, val FDEb: 2.1388,val WSFDE: 1.3183\n",
      "epoch spend time: 31.2895\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 113\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.8488\n",
      "Train Iter: 040, Train Loss: 0.6470\n",
      "Train Iter: 080, Train Loss: 0.6223\n",
      "Train Iter: 120, Train Loss: 0.7263\n",
      "val ADEv: 1.1348, val ADEp: 0.4356, val ADEb: 1.2361, val WSADE: 0.7516, val FDEv: 1.9710, val FDEp: 0.7705, val FDEb: 2.1098,val WSFDE: 1.3053\n",
      "epoch spend time: 31.0956\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 114\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6835\n",
      "Train Iter: 040, Train Loss: 0.4775\n",
      "Train Iter: 080, Train Loss: 0.8333\n",
      "Train Iter: 120, Train Loss: 0.6488\n",
      "val ADEv: 1.1311, val ADEp: 0.4423, val ADEb: 1.2449, val WSADE: 0.7566, val FDEv: 1.9676, val FDEp: 0.7887, val FDEb: 2.1482,val WSFDE: 1.3236\n",
      "epoch spend time: 31.1078\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 115\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6867\n",
      "Train Iter: 040, Train Loss: 0.6310\n",
      "Train Iter: 080, Train Loss: 0.8387\n",
      "Train Iter: 120, Train Loss: 0.5195\n",
      "val ADEv: 1.1312, val ADEp: 0.4404, val ADEb: 1.2451, val WSADE: 0.7556, val FDEv: 1.9663, val FDEp: 0.7839, val FDEb: 2.1311,val WSFDE: 1.3167\n",
      "epoch spend time: 31.2216\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 116\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.7604\n",
      "Train Iter: 040, Train Loss: 0.5994\n",
      "Train Iter: 080, Train Loss: 0.5741\n",
      "Train Iter: 120, Train Loss: 0.4903\n",
      "val ADEv: 1.1363, val ADEp: 0.4472, val ADEb: 1.2566, val WSADE: 0.7631, val FDEv: 1.9889, val FDEp: 0.7961, val FDEb: 2.1630,val WSFDE: 1.3354\n",
      "epoch spend time: 31.3188\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 117\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6294\n",
      "Train Iter: 040, Train Loss: 0.8167\n",
      "Train Iter: 080, Train Loss: 0.6829\n",
      "Train Iter: 120, Train Loss: 0.7135\n",
      "val ADEv: 1.1393, val ADEp: 0.4448, val ADEb: 1.2454, val WSADE: 0.7598, val FDEv: 1.9848, val FDEp: 0.7914, val FDEb: 2.1304,val WSFDE: 1.3246\n",
      "epoch spend time: 31.0089\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 118\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.5580\n",
      "Train Iter: 040, Train Loss: 0.9037\n",
      "Train Iter: 080, Train Loss: 0.7362\n",
      "Train Iter: 120, Train Loss: 0.7205\n",
      "val ADEv: 1.1259, val ADEp: 0.4405, val ADEb: 1.2279, val WSADE: 0.7508, val FDEv: 1.9575, val FDEp: 0.7836, val FDEb: 2.1091,val WSFDE: 1.3100\n",
      "epoch spend time: 31.0327\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 119\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6006\n",
      "Train Iter: 040, Train Loss: 0.6733\n",
      "Train Iter: 080, Train Loss: 0.5959\n",
      "Train Iter: 120, Train Loss: 0.5457\n",
      "val ADEv: 1.1266, val ADEp: 0.4390, val ADEb: 1.2411, val WSADE: 0.7530, val FDEv: 1.9561, val FDEp: 0.7787, val FDEb: 2.1246,val WSFDE: 1.3103\n",
      "epoch spend time: 30.6758\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 120\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6087\n",
      "Train Iter: 040, Train Loss: 0.7154\n",
      "Train Iter: 080, Train Loss: 0.7501\n",
      "Train Iter: 120, Train Loss: 0.7622\n",
      "val ADEv: 1.1422, val ADEp: 0.4430, val ADEb: 1.2552, val WSADE: 0.7615, val FDEv: 1.9923, val FDEp: 0.7899, val FDEb: 2.1603,val WSFDE: 1.3318\n",
      "epoch spend time: 30.6165\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 121\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6105\n",
      "Train Iter: 040, Train Loss: 0.6722\n",
      "Train Iter: 080, Train Loss: 0.6268\n",
      "Train Iter: 120, Train Loss: 0.7537\n",
      "val ADEv: 1.1470, val ADEp: 0.4400, val ADEb: 1.2300, val WSADE: 0.7552, val FDEv: 1.9932, val FDEp: 0.7806, val FDEb: 2.0989,val WSFDE: 1.3132\n",
      "epoch spend time: 31.2206\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 122\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6695\n",
      "Train Iter: 040, Train Loss: 0.6641\n",
      "Train Iter: 080, Train Loss: 0.6345\n",
      "Train Iter: 120, Train Loss: 0.6178\n",
      "val ADEv: 1.1281, val ADEp: 0.4486, val ADEb: 1.2400, val WSADE: 0.7586, val FDEv: 1.9618, val FDEp: 0.7928, val FDEb: 2.1266,val WSFDE: 1.3201\n",
      "epoch spend time: 31.1786\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 123\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.5942\n",
      "Train Iter: 040, Train Loss: 0.6623\n",
      "Train Iter: 080, Train Loss: 0.6723\n",
      "Train Iter: 120, Train Loss: 0.6128\n",
      "val ADEv: 1.1379, val ADEp: 0.4487, val ADEb: 1.2402, val WSADE: 0.7606, val FDEv: 1.9757, val FDEp: 0.7980, val FDEb: 2.1393,val WSFDE: 1.3286\n",
      "epoch spend time: 31.1898\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 124\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6553\n",
      "Train Iter: 040, Train Loss: 0.6189\n",
      "Train Iter: 080, Train Loss: 0.6034\n",
      "Train Iter: 120, Train Loss: 0.7100\n",
      "val ADEv: 1.1281, val ADEp: 0.4506, val ADEb: 1.2343, val WSADE: 0.7585, val FDEv: 1.9595, val FDEp: 0.8064, val FDEb: 2.1193,val WSFDE: 1.3259\n",
      "epoch spend time: 31.3141\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 125\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.5943\n",
      "Train Iter: 040, Train Loss: 0.5291\n",
      "Train Iter: 080, Train Loss: 0.6114\n",
      "Train Iter: 120, Train Loss: 0.8193\n",
      "val ADEv: 1.1354, val ADEp: 0.4497, val ADEb: 1.2642, val WSADE: 0.7660, val FDEv: 1.9776, val FDEp: 0.8028, val FDEb: 2.1760,val WSFDE: 1.3399\n",
      "epoch spend time: 31.3432\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 126\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6307\n",
      "Train Iter: 040, Train Loss: 0.6843\n",
      "Train Iter: 080, Train Loss: 0.7135\n",
      "Train Iter: 120, Train Loss: 0.8167\n",
      "val ADEv: 1.1359, val ADEp: 0.4482, val ADEb: 1.2475, val WSADE: 0.7616, val FDEv: 1.9691, val FDEp: 0.7982, val FDEb: 2.1359,val WSFDE: 1.3267\n",
      "epoch spend time: 30.8598\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 127\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.5864\n",
      "Train Iter: 040, Train Loss: 0.7072\n",
      "Train Iter: 080, Train Loss: 0.6566\n",
      "Train Iter: 120, Train Loss: 0.6489\n",
      "val ADEv: 1.1455, val ADEp: 0.4427, val ADEb: 1.2397, val WSADE: 0.7586, val FDEv: 1.9928, val FDEp: 0.7861, val FDEb: 2.1364,val WSFDE: 1.3245\n",
      "epoch spend time: 30.7890\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 128\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.5603\n",
      "Train Iter: 040, Train Loss: 0.6736\n",
      "Train Iter: 080, Train Loss: 0.7468\n",
      "Train Iter: 120, Train Loss: 0.5557\n",
      "val ADEv: 1.1234, val ADEp: 0.4543, val ADEb: 1.2527, val WSADE: 0.7637, val FDEv: 1.9521, val FDEp: 0.8110, val FDEb: 2.1393,val WSFDE: 1.3315\n",
      "epoch spend time: 31.0482\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 129\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6762\n",
      "Train Iter: 040, Train Loss: 0.6712\n",
      "Train Iter: 080, Train Loss: 0.6006\n",
      "Train Iter: 120, Train Loss: 0.6997\n",
      "val ADEv: 1.1263, val ADEp: 0.4459, val ADEb: 1.2465, val WSADE: 0.7581, val FDEv: 1.9576, val FDEp: 0.7961, val FDEb: 2.1344,val WSFDE: 1.3228\n",
      "epoch spend time: 30.7512\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 130\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6803\n",
      "Train Iter: 040, Train Loss: 0.7173\n",
      "Train Iter: 080, Train Loss: 0.7931\n",
      "Train Iter: 120, Train Loss: 0.4961\n",
      "val ADEv: 1.1292, val ADEp: 0.4449, val ADEb: 1.2522, val WSADE: 0.7594, val FDEv: 1.9706, val FDEp: 0.7946, val FDEb: 2.1612,val WSFDE: 1.3304\n",
      "epoch spend time: 30.9980\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 131\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.6932\n",
      "Train Iter: 040, Train Loss: 0.6565\n",
      "Train Iter: 080, Train Loss: 0.6662\n",
      "Train Iter: 120, Train Loss: 0.6743\n",
      "val ADEv: 1.1362, val ADEp: 0.4472, val ADEb: 1.2564, val WSADE: 0.7630, val FDEv: 1.9855, val FDEp: 0.7991, val FDEb: 2.1642,val WSFDE: 1.3367\n",
      "epoch spend time: 31.1629\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 132\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.5862\n",
      "Train Iter: 040, Train Loss: 0.7128\n",
      "Train Iter: 080, Train Loss: 0.6518\n",
      "Train Iter: 120, Train Loss: 0.4502\n",
      "val ADEv: 1.1228, val ADEp: 0.4403, val ADEb: 1.2209, val WSADE: 0.7485, val FDEv: 1.9479, val FDEp: 0.7834, val FDEb: 2.0741,val WSFDE: 1.3003\n",
      "epoch spend time: 31.2237\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 133\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.5764\n",
      "Train Iter: 040, Train Loss: 0.6831\n",
      "Train Iter: 080, Train Loss: 0.7208\n",
      "Train Iter: 120, Train Loss: 0.7270\n",
      "val ADEv: 1.1241, val ADEp: 0.4420, val ADEb: 1.2293, val WSADE: 0.7516, val FDEv: 1.9518, val FDEp: 0.7857, val FDEb: 2.1094,val WSFDE: 1.3102\n",
      "epoch spend time: 31.3846\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 134\n",
      "Learning Rate: 0.0005\n",
      "Train Iter: 000, Train Loss: 0.7298\n",
      "Train Iter: 040, Train Loss: 0.6007\n",
      "Train Iter: 080, Train Loss: 0.5839\n",
      "Train Iter: 120, Train Loss: 0.9103\n",
      "val ADEv: 1.1161, val ADEp: 0.4414, val ADEb: 1.2309, val WSADE: 0.7500, val FDEv: 1.9361, val FDEp: 0.7797, val FDEb: 2.1016,val WSFDE: 1.3018\n",
      "epoch spend time: 31.4335\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 135\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.6384\n",
      "Train Iter: 040, Train Loss: 0.8512\n",
      "Train Iter: 080, Train Loss: 0.5499\n",
      "Train Iter: 120, Train Loss: 0.6261\n",
      "val ADEv: 1.1118, val ADEp: 0.4416, val ADEb: 1.2219, val WSADE: 0.7473, val FDEv: 1.9344, val FDEp: 0.7882, val FDEb: 2.0951,val WSFDE: 1.3050\n",
      "epoch spend time: 30.5879\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 136\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.7088\n",
      "Train Iter: 040, Train Loss: 0.7187\n",
      "Train Iter: 080, Train Loss: 0.6224\n",
      "Train Iter: 120, Train Loss: 0.6387\n",
      "val ADEv: 1.1088, val ADEp: 0.4369, val ADEb: 1.2314, val WSADE: 0.7460, val FDEv: 1.9255, val FDEp: 0.7764, val FDEb: 2.1089,val WSFDE: 1.2994\n",
      "epoch spend time: 31.3050\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 137\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.7814\n",
      "Train Iter: 040, Train Loss: 0.7463\n",
      "Train Iter: 080, Train Loss: 0.5780\n",
      "Train Iter: 120, Train Loss: 0.7297\n",
      "val ADEv: 1.1204, val ADEp: 0.4410, val ADEb: 1.2667, val WSADE: 0.7585, val FDEv: 1.9491, val FDEp: 0.7861, val FDEb: 2.1738,val WSFDE: 1.3240\n",
      "epoch spend time: 31.1028\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 138\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.6628\n",
      "Train Iter: 040, Train Loss: 0.7226\n",
      "Train Iter: 080, Train Loss: 0.6617\n",
      "Train Iter: 120, Train Loss: 0.5910\n",
      "val ADEv: 1.1105, val ADEp: 0.4351, val ADEb: 1.2179, val WSADE: 0.7424, val FDEv: 1.9237, val FDEp: 0.7712, val FDEb: 2.0751,val WSFDE: 1.2885\n",
      "epoch spend time: 31.1574\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 139\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.6491\n",
      "Train Iter: 040, Train Loss: 0.6999\n",
      "Train Iter: 080, Train Loss: 0.7602\n",
      "Train Iter: 120, Train Loss: 0.5158\n",
      "val ADEv: 1.1222, val ADEp: 0.4421, val ADEb: 1.2532, val WSADE: 0.7566, val FDEv: 1.9558, val FDEp: 0.7893, val FDEb: 2.1517,val WSFDE: 1.3223\n",
      "epoch spend time: 30.9815\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 140\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.6420\n",
      "Train Iter: 040, Train Loss: 0.4999\n",
      "Train Iter: 080, Train Loss: 0.6176\n",
      "Train Iter: 120, Train Loss: 0.5698\n",
      "val ADEv: 1.1286, val ADEp: 0.4360, val ADEb: 1.2166, val WSADE: 0.7462, val FDEv: 1.9552, val FDEp: 0.7708, val FDEb: 2.0779,val WSFDE: 1.2952\n",
      "epoch spend time: 31.3805\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 141\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.7127\n",
      "Train Iter: 040, Train Loss: 0.6364\n",
      "Train Iter: 080, Train Loss: 0.6289\n",
      "Train Iter: 120, Train Loss: 0.5347\n",
      "val ADEv: 1.1138, val ADEp: 0.4424, val ADEb: 1.2134, val WSADE: 0.7463, val FDEv: 1.9342, val FDEp: 0.7869, val FDEb: 2.0748,val WSFDE: 1.2997\n",
      "epoch spend time: 31.0452\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 142\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.5907\n",
      "Train Iter: 040, Train Loss: 0.6100\n",
      "Train Iter: 080, Train Loss: 0.7563\n",
      "Train Iter: 120, Train Loss: 0.7190\n",
      "val ADEv: 1.1048, val ADEp: 0.4424, val ADEb: 1.2027, val WSADE: 0.7421, val FDEv: 1.9178, val FDEp: 0.7885, val FDEb: 2.0500,val WSFDE: 1.2919\n",
      "epoch spend time: 31.4556\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 143\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.7908\n",
      "Train Iter: 040, Train Loss: 0.6383\n",
      "Train Iter: 080, Train Loss: 0.5351\n",
      "Train Iter: 120, Train Loss: 0.7471\n",
      "val ADEv: 1.1106, val ADEp: 0.4471, val ADEb: 1.2211, val WSADE: 0.7501, val FDEv: 1.9306, val FDEp: 0.7982, val FDEb: 2.0871,val WSFDE: 1.3083\n",
      "epoch spend time: 31.3595\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 144\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.7077\n",
      "Train Iter: 040, Train Loss: 0.7530\n",
      "Train Iter: 080, Train Loss: 0.5410\n",
      "Train Iter: 120, Train Loss: 0.7153\n",
      "val ADEv: 1.1043, val ADEp: 0.4377, val ADEb: 1.2176, val WSADE: 0.7426, val FDEv: 1.9111, val FDEp: 0.7764, val FDEb: 2.0732,val WSFDE: 1.2886\n",
      "epoch spend time: 31.2929\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 145\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.6257\n",
      "Train Iter: 040, Train Loss: 0.5767\n",
      "Train Iter: 080, Train Loss: 0.7268\n",
      "Train Iter: 120, Train Loss: 0.8017\n",
      "val ADEv: 1.1115, val ADEp: 0.4388, val ADEb: 1.2191, val WSADE: 0.7450, val FDEv: 1.9242, val FDEp: 0.7806, val FDEb: 2.0840,val WSFDE: 1.2960\n",
      "epoch spend time: 31.3130\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 146\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.6064\n",
      "Train Iter: 040, Train Loss: 0.5808\n",
      "Train Iter: 080, Train Loss: 0.6554\n",
      "Train Iter: 120, Train Loss: 0.6955\n",
      "val ADEv: 1.1105, val ADEp: 0.4368, val ADEb: 1.2193, val WSADE: 0.7437, val FDEv: 1.9265, val FDEp: 0.7768, val FDEb: 2.0868,val WSFDE: 1.2950\n",
      "epoch spend time: 31.1877\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 147\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.5032\n",
      "Train Iter: 040, Train Loss: 0.6755\n",
      "Train Iter: 080, Train Loss: 0.7774\n",
      "Train Iter: 120, Train Loss: 0.6398\n",
      "val ADEv: 1.1078, val ADEp: 0.4392, val ADEb: 1.2252, val WSADE: 0.7459, val FDEv: 1.9191, val FDEp: 0.7824, val FDEb: 2.1014,val WSFDE: 1.2999\n",
      "epoch spend time: 30.9669\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 148\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.5048\n",
      "Train Iter: 040, Train Loss: 0.7324\n",
      "Train Iter: 080, Train Loss: 0.7083\n",
      "Train Iter: 120, Train Loss: 0.6825\n",
      "val ADEv: 1.1102, val ADEp: 0.4374, val ADEb: 1.2280, val WSADE: 0.7459, val FDEv: 1.9251, val FDEp: 0.7805, val FDEb: 2.1080,val WSFDE: 1.3015\n",
      "epoch spend time: 31.3085\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 149\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.5877\n",
      "Train Iter: 040, Train Loss: 0.6723\n",
      "Train Iter: 080, Train Loss: 0.5288\n",
      "Train Iter: 120, Train Loss: 0.6198\n",
      "val ADEv: 1.1089, val ADEp: 0.4385, val ADEb: 1.2220, val WSADE: 0.7449, val FDEv: 1.9158, val FDEp: 0.7817, val FDEb: 2.0824,val WSFDE: 1.2947\n",
      "epoch spend time: 31.1547\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 150\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.5165\n",
      "Train Iter: 040, Train Loss: 0.6366\n",
      "Train Iter: 080, Train Loss: 0.7018\n",
      "Train Iter: 120, Train Loss: 0.6885\n",
      "val ADEv: 1.1161, val ADEp: 0.4413, val ADEb: 1.2266, val WSADE: 0.7490, val FDEv: 1.9372, val FDEp: 0.7866, val FDEb: 2.1096,val WSFDE: 1.3078\n",
      "epoch spend time: 31.2086\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 151\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.6417\n",
      "Train Iter: 040, Train Loss: 0.5395\n",
      "Train Iter: 080, Train Loss: 0.6479\n",
      "Train Iter: 120, Train Loss: 0.8808\n",
      "val ADEv: 1.1091, val ADEp: 0.4344, val ADEb: 1.2221, val WSADE: 0.7426, val FDEv: 1.9240, val FDEp: 0.7718, val FDEb: 2.0840,val WSFDE: 1.2910\n",
      "epoch spend time: 30.9617\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 152\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.7844\n",
      "Train Iter: 040, Train Loss: 0.7884\n",
      "Train Iter: 080, Train Loss: 0.4930\n",
      "Train Iter: 120, Train Loss: 0.8285\n",
      "val ADEv: 1.1176, val ADEp: 0.4379, val ADEb: 1.2373, val WSADE: 0.7497, val FDEv: 1.9393, val FDEp: 0.7806, val FDEb: 2.1178,val WSFDE: 1.3065\n",
      "epoch spend time: 31.0720\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 153\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.5917\n",
      "Train Iter: 040, Train Loss: 0.7450\n",
      "Train Iter: 080, Train Loss: 0.6489\n",
      "Train Iter: 120, Train Loss: 0.7123\n",
      "val ADEv: 1.1041, val ADEp: 0.4424, val ADEb: 1.2282, val WSADE: 0.7476, val FDEv: 1.9133, val FDEp: 0.7889, val FDEb: 2.1120,val WSFDE: 1.3048\n",
      "epoch spend time: 31.0923\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 154\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.5661\n",
      "Train Iter: 040, Train Loss: 0.6200\n",
      "Train Iter: 080, Train Loss: 0.4534\n",
      "Train Iter: 120, Train Loss: 0.8250\n",
      "val ADEv: 1.1040, val ADEp: 0.4377, val ADEb: 1.2204, val WSADE: 0.7431, val FDEv: 1.9145, val FDEp: 0.7803, val FDEb: 2.0836,val WSFDE: 1.2939\n",
      "epoch spend time: 30.8054\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 155\n",
      "Learning Rate: 0.00025\n",
      "Train Iter: 000, Train Loss: 0.7292\n",
      "Train Iter: 040, Train Loss: 0.6079\n",
      "Train Iter: 080, Train Loss: 0.7256\n",
      "Train Iter: 120, Train Loss: 0.6643\n",
      "val ADEv: 1.1159, val ADEp: 0.4370, val ADEb: 1.2261, val WSADE: 0.7464, val FDEv: 1.9381, val FDEp: 0.7772, val FDEb: 2.0977,val WSFDE: 1.2999\n",
      "epoch spend time: 31.0834\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 156\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.7411\n",
      "Train Iter: 040, Train Loss: 0.6389\n",
      "Train Iter: 080, Train Loss: 0.7945\n",
      "Train Iter: 120, Train Loss: 0.7319\n",
      "val ADEv: 1.1140, val ADEp: 0.4393, val ADEb: 1.2255, val WSADE: 0.7472, val FDEv: 1.9352, val FDEp: 0.7824, val FDEb: 2.1011,val WSFDE: 1.3031\n",
      "epoch spend time: 30.8807\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 157\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6848\n",
      "Train Iter: 040, Train Loss: 0.6849\n",
      "Train Iter: 080, Train Loss: 0.7226\n",
      "Train Iter: 120, Train Loss: 0.6559\n",
      "val ADEv: 1.1072, val ADEp: 0.4387, val ADEb: 1.2309, val WSADE: 0.7467, val FDEv: 1.9206, val FDEp: 0.7829, val FDEb: 2.1030,val WSFDE: 1.3009\n",
      "epoch spend time: 31.0706\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 158\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5631\n",
      "Train Iter: 040, Train Loss: 0.6307\n",
      "Train Iter: 080, Train Loss: 0.5303\n",
      "Train Iter: 120, Train Loss: 0.6714\n",
      "val ADEv: 1.1111, val ADEp: 0.4387, val ADEb: 1.2137, val WSADE: 0.7437, val FDEv: 1.9253, val FDEp: 0.7820, val FDEb: 2.0693,val WSFDE: 1.2939\n",
      "epoch spend time: 30.9090\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 159\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6489\n",
      "Train Iter: 040, Train Loss: 0.7321\n",
      "Train Iter: 080, Train Loss: 0.7894\n",
      "Train Iter: 120, Train Loss: 0.6262\n",
      "val ADEv: 1.1029, val ADEp: 0.4369, val ADEb: 1.2212, val WSADE: 0.7427, val FDEv: 1.9085, val FDEp: 0.7779, val FDEb: 2.0870,val WSFDE: 1.2921\n",
      "epoch spend time: 31.2170\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 160\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.4818\n",
      "Train Iter: 040, Train Loss: 0.6875\n",
      "Train Iter: 080, Train Loss: 0.9474\n",
      "Train Iter: 120, Train Loss: 0.5883\n",
      "val ADEv: 1.1123, val ADEp: 0.4365, val ADEb: 1.2219, val WSADE: 0.7445, val FDEv: 1.9290, val FDEp: 0.7757, val FDEb: 2.0810,val WSFDE: 1.2935\n",
      "epoch spend time: 31.0962\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 161\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6516\n",
      "Train Iter: 040, Train Loss: 0.5922\n",
      "Train Iter: 080, Train Loss: 0.6850\n",
      "Train Iter: 120, Train Loss: 0.6296\n",
      "val ADEv: 1.1027, val ADEp: 0.4371, val ADEb: 1.2213, val WSADE: 0.7427, val FDEv: 1.9102, val FDEp: 0.7782, val FDEb: 2.0852,val WSFDE: 1.2921\n",
      "epoch spend time: 31.2067\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 162\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.7052\n",
      "Train Iter: 040, Train Loss: 0.5119\n",
      "Train Iter: 080, Train Loss: 0.5730\n",
      "Train Iter: 120, Train Loss: 0.5353\n",
      "val ADEv: 1.1051, val ADEp: 0.4370, val ADEb: 1.2345, val WSADE: 0.7460, val FDEv: 1.9146, val FDEp: 0.7788, val FDEb: 2.1122,val WSFDE: 1.2993\n",
      "epoch spend time: 31.2542\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 163\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6801\n",
      "Train Iter: 040, Train Loss: 0.5230\n",
      "Train Iter: 080, Train Loss: 0.6149\n",
      "Train Iter: 120, Train Loss: 0.7466\n",
      "val ADEv: 1.1024, val ADEp: 0.4345, val ADEb: 1.2187, val WSADE: 0.7406, val FDEv: 1.9081, val FDEp: 0.7723, val FDEb: 2.0655,val WSFDE: 1.2840\n",
      "epoch spend time: 31.2091\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 164\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6570\n",
      "Train Iter: 040, Train Loss: 0.6559\n",
      "Train Iter: 080, Train Loss: 0.6498\n",
      "Train Iter: 120, Train Loss: 0.6345\n",
      "val ADEv: 1.1063, val ADEp: 0.4350, val ADEb: 1.2197, val WSADE: 0.7419, val FDEv: 1.9144, val FDEp: 0.7740, val FDEb: 2.0748,val WSFDE: 1.2882\n",
      "epoch spend time: 31.4475\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 165\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5683\n",
      "Train Iter: 040, Train Loss: 0.6076\n",
      "Train Iter: 080, Train Loss: 0.7096\n",
      "Train Iter: 120, Train Loss: 0.6857\n",
      "val ADEv: 1.1059, val ADEp: 0.4355, val ADEb: 1.2145, val WSADE: 0.7410, val FDEv: 1.9160, val FDEp: 0.7735, val FDEb: 2.0689,val WSFDE: 1.2870\n",
      "epoch spend time: 31.1478\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 166\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6199\n",
      "Train Iter: 040, Train Loss: 0.6465\n",
      "Train Iter: 080, Train Loss: 0.8038\n",
      "Train Iter: 120, Train Loss: 0.6573\n",
      "val ADEv: 1.1067, val ADEp: 0.4352, val ADEb: 1.2139, val WSADE: 0.7408, val FDEv: 1.9147, val FDEp: 0.7745, val FDEb: 2.0663,val WSFDE: 1.2868\n",
      "epoch spend time: 31.3165\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 167\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5193\n",
      "Train Iter: 040, Train Loss: 0.6591\n",
      "Train Iter: 080, Train Loss: 0.7151\n",
      "Train Iter: 120, Train Loss: 0.7371\n",
      "val ADEv: 1.1070, val ADEp: 0.4378, val ADEb: 1.2185, val WSADE: 0.7434, val FDEv: 1.9165, val FDEp: 0.7809, val FDEb: 2.0823,val WSFDE: 1.2943\n",
      "epoch spend time: 31.1495\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 168\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6249\n",
      "Train Iter: 040, Train Loss: 0.5646\n",
      "Train Iter: 080, Train Loss: 0.6710\n",
      "Train Iter: 120, Train Loss: 0.4708\n",
      "val ADEv: 1.1085, val ADEp: 0.4386, val ADEb: 1.2308, val WSADE: 0.7469, val FDEv: 1.9164, val FDEp: 0.7810, val FDEb: 2.1079,val WSFDE: 1.3000\n",
      "epoch spend time: 31.3244\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 169\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5800\n",
      "Train Iter: 040, Train Loss: 0.6401\n",
      "Train Iter: 080, Train Loss: 0.6145\n",
      "Train Iter: 120, Train Loss: 0.6950\n",
      "val ADEv: 1.1073, val ADEp: 0.4363, val ADEb: 1.2310, val WSADE: 0.7453, val FDEv: 1.9156, val FDEp: 0.7767, val FDEb: 2.1071,val WSFDE: 1.2972\n",
      "epoch spend time: 31.1754\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 170\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.7515\n",
      "Train Iter: 040, Train Loss: 0.6478\n",
      "Train Iter: 080, Train Loss: 0.6011\n",
      "Train Iter: 120, Train Loss: 0.7580\n",
      "val ADEv: 1.1074, val ADEp: 0.4384, val ADEb: 1.2304, val WSADE: 0.7465, val FDEv: 1.9172, val FDEp: 0.7819, val FDEb: 2.1003,val WSFDE: 1.2990\n",
      "epoch spend time: 30.9630\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 171\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6850\n",
      "Train Iter: 040, Train Loss: 0.7106\n",
      "Train Iter: 080, Train Loss: 0.5652\n",
      "Train Iter: 120, Train Loss: 0.5264\n",
      "val ADEv: 1.0968, val ADEp: 0.4343, val ADEb: 1.2247, val WSADE: 0.7407, val FDEv: 1.8965, val FDEp: 0.7737, val FDEb: 2.0981,val WSFDE: 1.2896\n",
      "epoch spend time: 30.8716\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 172\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6793\n",
      "Train Iter: 040, Train Loss: 0.6981\n",
      "Train Iter: 080, Train Loss: 0.5344\n",
      "Train Iter: 120, Train Loss: 0.5567\n",
      "val ADEv: 1.1054, val ADEp: 0.4368, val ADEb: 1.2356, val WSADE: 0.7463, val FDEv: 1.9115, val FDEp: 0.7796, val FDEb: 2.1111,val WSFDE: 1.2989\n",
      "epoch spend time: 31.2496\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 173\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5875\n",
      "Train Iter: 040, Train Loss: 0.8321\n",
      "Train Iter: 080, Train Loss: 0.7040\n",
      "Train Iter: 120, Train Loss: 0.7461\n",
      "val ADEv: 1.1023, val ADEp: 0.4341, val ADEb: 1.2178, val WSADE: 0.7402, val FDEv: 1.9028, val FDEp: 0.7714, val FDEb: 2.0758,val WSFDE: 1.2846\n",
      "epoch spend time: 30.9359\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 174\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5152\n",
      "Train Iter: 040, Train Loss: 0.7187\n",
      "Train Iter: 080, Train Loss: 0.4472\n",
      "Train Iter: 120, Train Loss: 0.6847\n",
      "val ADEv: 1.1009, val ADEp: 0.4332, val ADEb: 1.2241, val WSADE: 0.7408, val FDEv: 1.9027, val FDEp: 0.7689, val FDEb: 2.0818,val WSFDE: 1.2845\n",
      "epoch spend time: 31.1400\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 175\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.7319\n",
      "Train Iter: 040, Train Loss: 0.5187\n",
      "Train Iter: 080, Train Loss: 0.5247\n",
      "Train Iter: 120, Train Loss: 0.6949\n",
      "val ADEv: 1.1041, val ADEp: 0.4376, val ADEb: 1.2249, val WSADE: 0.7441, val FDEv: 1.9135, val FDEp: 0.7802, val FDEb: 2.0971,val WSFDE: 1.2966\n",
      "epoch spend time: 31.2312\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 176\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6481\n",
      "Train Iter: 040, Train Loss: 0.6162\n",
      "Train Iter: 080, Train Loss: 0.4985\n",
      "Train Iter: 120, Train Loss: 0.6943\n",
      "val ADEv: 1.0964, val ADEp: 0.4322, val ADEb: 1.2150, val WSADE: 0.7372, val FDEv: 1.8945, val FDEp: 0.7672, val FDEb: 2.0713,val WSFDE: 1.2796\n",
      "epoch spend time: 30.7307\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 177\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5842\n",
      "Train Iter: 040, Train Loss: 0.5933\n",
      "Train Iter: 080, Train Loss: 0.6106\n",
      "Train Iter: 120, Train Loss: 0.7255\n",
      "val ADEv: 1.0972, val ADEp: 0.4384, val ADEb: 1.2164, val WSADE: 0.7413, val FDEv: 1.8962, val FDEp: 0.7806, val FDEb: 2.0838,val WSFDE: 1.2904\n",
      "epoch spend time: 31.1477\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 178\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5554\n",
      "Train Iter: 040, Train Loss: 0.5993\n",
      "Train Iter: 080, Train Loss: 0.5723\n",
      "Train Iter: 120, Train Loss: 0.5239\n",
      "val ADEv: 1.0992, val ADEp: 0.4365, val ADEb: 1.2209, val WSADE: 0.7416, val FDEv: 1.9009, val FDEp: 0.7772, val FDEb: 2.0827,val WSFDE: 1.2891\n",
      "epoch spend time: 31.0364\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 179\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5736\n",
      "Train Iter: 040, Train Loss: 0.7664\n",
      "Train Iter: 080, Train Loss: 0.6240\n",
      "Train Iter: 120, Train Loss: 0.5431\n",
      "val ADEv: 1.1006, val ADEp: 0.4337, val ADEb: 1.2171, val WSADE: 0.7394, val FDEv: 1.9024, val FDEp: 0.7698, val FDEb: 2.0718,val WSFDE: 1.2828\n",
      "epoch spend time: 30.3328\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 180\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.7727\n",
      "Train Iter: 040, Train Loss: 0.5255\n",
      "Train Iter: 080, Train Loss: 0.6406\n",
      "Train Iter: 120, Train Loss: 0.5569\n",
      "val ADEv: 1.0983, val ADEp: 0.4369, val ADEb: 1.2192, val WSADE: 0.7413, val FDEv: 1.8999, val FDEp: 0.7778, val FDEb: 2.0799,val WSFDE: 1.2887\n",
      "epoch spend time: 30.9765\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 181\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6858\n",
      "Train Iter: 040, Train Loss: 0.6559\n",
      "Train Iter: 080, Train Loss: 0.7444\n",
      "Train Iter: 120, Train Loss: 0.6144\n",
      "val ADEv: 1.1056, val ADEp: 0.4385, val ADEb: 1.2330, val WSADE: 0.7467, val FDEv: 1.9167, val FDEp: 0.7824, val FDEb: 2.1111,val WSFDE: 1.3016\n",
      "epoch spend time: 30.8687\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 182\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5955\n",
      "Train Iter: 040, Train Loss: 0.5939\n",
      "Train Iter: 080, Train Loss: 0.6677\n",
      "Train Iter: 120, Train Loss: 0.5951\n",
      "val ADEv: 1.0969, val ADEp: 0.4371, val ADEb: 1.2205, val WSADE: 0.7414, val FDEv: 1.8963, val FDEp: 0.7782, val FDEb: 2.0847,val WSFDE: 1.2892\n",
      "epoch spend time: 31.0612\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 183\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.5571\n",
      "Train Iter: 040, Train Loss: 0.8002\n",
      "Train Iter: 080, Train Loss: 0.5939\n",
      "Train Iter: 120, Train Loss: 0.7685\n",
      "val ADEv: 1.1007, val ADEp: 0.4361, val ADEb: 1.2208, val WSADE: 0.7417, val FDEv: 1.9059, val FDEp: 0.7760, val FDEb: 2.0793,val WSFDE: 1.2887\n",
      "epoch spend time: 30.4416\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 184\n",
      "Learning Rate: 0.000125\n",
      "Train Iter: 000, Train Loss: 0.6680\n",
      "Train Iter: 040, Train Loss: 0.6972\n",
      "Train Iter: 080, Train Loss: 0.6991\n",
      "Train Iter: 120, Train Loss: 0.5576\n",
      "val ADEv: 1.0974, val ADEp: 0.4339, val ADEb: 1.2164, val WSADE: 0.7387, val FDEv: 1.8985, val FDEp: 0.7724, val FDEb: 2.0643,val WSFDE: 1.2818\n",
      "epoch spend time: 30.3583\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 185\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.6500\n",
      "Train Iter: 040, Train Loss: 0.6090\n",
      "Train Iter: 080, Train Loss: 0.5715\n",
      "Train Iter: 120, Train Loss: 0.6137\n",
      "val ADEv: 1.1029, val ADEp: 0.4340, val ADEb: 1.2077, val WSADE: 0.7380, val FDEv: 1.9097, val FDEp: 0.7716, val FDEb: 2.0580,val WSFDE: 1.2822\n",
      "epoch spend time: 30.4446\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 186\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.5331\n",
      "Train Iter: 040, Train Loss: 0.5701\n",
      "Train Iter: 080, Train Loss: 0.7380\n",
      "Train Iter: 120, Train Loss: 0.7936\n",
      "val ADEv: 1.1005, val ADEp: 0.4342, val ADEb: 1.2187, val WSADE: 0.7400, val FDEv: 1.9054, val FDEp: 0.7728, val FDEb: 2.0763,val WSFDE: 1.2861\n",
      "epoch spend time: 30.7345\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 187\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.6642\n",
      "Train Iter: 040, Train Loss: 0.4730\n",
      "Train Iter: 080, Train Loss: 0.6644\n",
      "Train Iter: 120, Train Loss: 0.5643\n",
      "val ADEv: 1.1004, val ADEp: 0.4370, val ADEb: 1.2267, val WSADE: 0.7434, val FDEv: 1.9070, val FDEp: 0.7789, val FDEb: 2.0899,val WSFDE: 1.2929\n",
      "epoch spend time: 30.9778\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 188\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.6111\n",
      "Train Iter: 040, Train Loss: 0.5254\n",
      "Train Iter: 080, Train Loss: 0.7732\n",
      "Train Iter: 120, Train Loss: 0.6154\n",
      "val ADEv: 1.0993, val ADEp: 0.4337, val ADEb: 1.2138, val WSADE: 0.7385, val FDEv: 1.9022, val FDEp: 0.7705, val FDEb: 2.0611,val WSFDE: 1.2808\n",
      "epoch spend time: 30.6968\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 189\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.7719\n",
      "Train Iter: 040, Train Loss: 0.5356\n",
      "Train Iter: 080, Train Loss: 0.7708\n",
      "Train Iter: 120, Train Loss: 0.6699\n",
      "val ADEv: 1.1020, val ADEp: 0.4353, val ADEb: 1.2288, val WSADE: 0.7432, val FDEv: 1.9086, val FDEp: 0.7754, val FDEb: 2.1030,val WSFDE: 1.2941\n",
      "epoch spend time: 30.8116\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 190\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.7471\n",
      "Train Iter: 040, Train Loss: 0.8699\n",
      "Train Iter: 080, Train Loss: 0.7110\n",
      "Train Iter: 120, Train Loss: 0.5894\n",
      "val ADEv: 1.0964, val ADEp: 0.4356, val ADEb: 1.2140, val WSADE: 0.7390, val FDEv: 1.8933, val FDEp: 0.7753, val FDEb: 2.0729,val WSFDE: 1.2844\n",
      "epoch spend time: 30.7893\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 191\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.6603\n",
      "Train Iter: 040, Train Loss: 0.5764\n",
      "Train Iter: 080, Train Loss: 0.5796\n",
      "Train Iter: 120, Train Loss: 0.7034\n",
      "val ADEv: 1.1013, val ADEp: 0.4348, val ADEb: 1.2155, val WSADE: 0.7399, val FDEv: 1.9018, val FDEp: 0.7733, val FDEb: 2.0696,val WSFDE: 1.2842\n",
      "epoch spend time: 30.6860\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 192\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.8531\n",
      "Train Iter: 040, Train Loss: 0.4575\n",
      "Train Iter: 080, Train Loss: 0.7348\n",
      "Train Iter: 120, Train Loss: 0.5867\n",
      "val ADEv: 1.1018, val ADEp: 0.4367, val ADEb: 1.2185, val WSADE: 0.7417, val FDEv: 1.9058, val FDEp: 0.7781, val FDEb: 2.0764,val WSFDE: 1.2892\n",
      "epoch spend time: 30.9399\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 193\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.7484\n",
      "Train Iter: 040, Train Loss: 0.6822\n",
      "Train Iter: 080, Train Loss: 0.6893\n",
      "Train Iter: 120, Train Loss: 0.5829\n",
      "val ADEv: 1.0975, val ADEp: 0.4343, val ADEb: 1.2212, val WSADE: 0.7401, val FDEv: 1.8981, val FDEp: 0.7726, val FDEb: 2.0782,val WSFDE: 1.2849\n",
      "epoch spend time: 30.7254\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 194\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.6531\n",
      "Train Iter: 040, Train Loss: 0.6475\n",
      "Train Iter: 080, Train Loss: 0.6108\n",
      "Train Iter: 120, Train Loss: 0.6404\n",
      "val ADEv: 1.0950, val ADEp: 0.4337, val ADEb: 1.2156, val WSADE: 0.7380, val FDEv: 1.8925, val FDEp: 0.7716, val FDEb: 2.0721,val WSFDE: 1.2819\n",
      "epoch spend time: 30.8988\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 195\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.6019\n",
      "Train Iter: 040, Train Loss: 0.8044\n",
      "Train Iter: 080, Train Loss: 0.6712\n",
      "Train Iter: 120, Train Loss: 0.6359\n",
      "val ADEv: 1.0965, val ADEp: 0.4344, val ADEb: 1.2154, val WSADE: 0.7386, val FDEv: 1.8948, val FDEp: 0.7722, val FDEb: 2.0649,val WSFDE: 1.2811\n",
      "epoch spend time: 30.8864\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 196\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.6436\n",
      "Train Iter: 040, Train Loss: 0.6031\n",
      "Train Iter: 080, Train Loss: 0.5589\n",
      "Train Iter: 120, Train Loss: 0.5846\n",
      "val ADEv: 1.0969, val ADEp: 0.4341, val ADEb: 1.2095, val WSADE: 0.7373, val FDEv: 1.8925, val FDEp: 0.7720, val FDEb: 2.0599,val WSFDE: 1.2794\n",
      "epoch spend time: 31.1225\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 197\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.7045\n",
      "Train Iter: 040, Train Loss: 0.6433\n",
      "Train Iter: 080, Train Loss: 0.6575\n",
      "Train Iter: 120, Train Loss: 0.5035\n",
      "val ADEv: 1.0950, val ADEp: 0.4345, val ADEb: 1.2131, val WSADE: 0.7379, val FDEv: 1.8932, val FDEp: 0.7730, val FDEb: 2.0652,val WSFDE: 1.2813\n",
      "epoch spend time: 30.7402\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 198\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.5904\n",
      "Train Iter: 040, Train Loss: 0.5995\n",
      "Train Iter: 080, Train Loss: 0.5204\n",
      "Train Iter: 120, Train Loss: 0.6108\n",
      "val ADEv: 1.0969, val ADEp: 0.4334, val ADEb: 1.2157, val WSADE: 0.7382, val FDEv: 1.8961, val FDEp: 0.7706, val FDEb: 2.0680,val WSFDE: 1.2811\n",
      "epoch spend time: 30.7855\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 199\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.5378\n",
      "Train Iter: 040, Train Loss: 0.7660\n",
      "Train Iter: 080, Train Loss: 0.5703\n",
      "Train Iter: 120, Train Loss: 0.6952\n",
      "val ADEv: 1.0967, val ADEp: 0.4337, val ADEb: 1.2125, val WSADE: 0.7376, val FDEv: 1.8943, val FDEp: 0.7711, val FDEb: 2.0566,val WSFDE: 1.2786\n",
      "epoch spend time: 30.9952\n",
      "-----------------------------------------------------------\n",
      "Train start\n",
      "Epoch: 200\n",
      "Learning Rate: 6.25e-05\n",
      "Train Iter: 000, Train Loss: 0.5541\n",
      "Train Iter: 040, Train Loss: 0.5287\n",
      "Train Iter: 080, Train Loss: 0.5342\n",
      "Train Iter: 120, Train Loss: 0.6129\n",
      "val ADEv: 1.0942, val ADEp: 0.4346, val ADEb: 1.2101, val WSADE: 0.7372, val FDEv: 1.8911, val FDEp: 0.7736, val FDEb: 2.0613,val WSFDE: 1.2804\n",
      "epoch spend time: 30.9735\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_time = []\n",
    "best_wsade = []\n",
    "best_epoch = []\n",
    "best_wsfde = []\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"Train start\")\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f'Learning Rate: {param_group[\"lr\"]}')\n",
    "        \n",
    "    model.train()\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        features,masks,distance_adj,heading_adj,mean = batch_data\n",
    "        masks = masks.to(device)\n",
    "        distance_adj = distance_adj.to(device)\n",
    "        heading_adj = heading_adj.to(device)\n",
    "        # category_adj = category_adj.to(device)\n",
    "        features_x = features[:, :history_frames, :, :6].to(device) # (64,6,114,6)\n",
    "        features_y = features[:, history_frames:, :, :].to(device)  # (64,6,114,8)\n",
    "        future_masks = masks[:, history_frames:, :, :]  # (64,6,114,1)\n",
    "        \n",
    "        prediction = model(features_x,distance_adj,heading_adj)  # (64,6,114,2)\n",
    "        \n",
    "        loss = torch.sum(torch.abs(prediction*future_masks - features_y[:,:,:,:2]*future_masks))/torch.sum(future_masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 40 == 0:\n",
    "            log = 'Train Iter: {:03d}, Train Loss: {:.4f}'\n",
    "            print(log.format(i, loss.item()), flush=True)\n",
    "    \n",
    "    # val\n",
    "    all_overall_sum_list = []\n",
    "    all_overall_num_list = []\n",
    "    all_car_sum_list = []\n",
    "    all_car_num_list = []\n",
    "    all_human_sum_list = []\n",
    "    all_human_num_list = []\n",
    "    all_bike_sum_list = []\n",
    "    all_bike_num_list = []\n",
    "    model.eval()\n",
    "    for i, batch_data in enumerate(val_loader):\n",
    "        features,masks,distance_adj,heading_adj,mean = batch_data\n",
    "        masks = masks.to(device)\n",
    "        distance_adj = distance_adj.to(device)\n",
    "        heading_adj = heading_adj.to(device)\n",
    "        # category_adj = category_adj.to(device)\n",
    "        features_x = features[:, :history_frames, :, :6].to(device)  # (64,6,114,6)\n",
    "        features_y = features[:, history_frames:, :, :].to(device)  # (64,6,114,8)\n",
    "        \n",
    "        future_masks = masks[:, history_frames:, :, :]  # (64,6,114,1)\n",
    "        last_position = features[:,history_frames - 1,:,-2:].to(device)\n",
    "        \n",
    "        prediction = model(features_x,distance_adj,heading_adj)  # (64,6,114,2)\n",
    "        prediction = inverse_transform(prediction, last_position)\n",
    "        \n",
    "        a,b,c,d,e,f,g,h = eva(prediction,features_y,future_masks)\n",
    "        all_overall_num_list.extend(a)\n",
    "        all_overall_sum_list.extend(b)\n",
    "        all_car_num_list.extend(c)\n",
    "        all_car_sum_list.extend(d)\n",
    "        all_human_num_list.extend(e)\n",
    "        all_human_sum_list.extend(f)\n",
    "        all_bike_num_list.extend(g)\n",
    "        all_bike_sum_list.extend(h)\n",
    "        \n",
    "    result_car = display_result([np.array(all_car_sum_list), np.array(all_car_num_list)], pra_pref='car')\n",
    "    result_human = display_result([np.array(all_human_sum_list), np.array(all_human_num_list)], pra_pref='human')\n",
    "    result_bike = display_result([np.array(all_bike_sum_list), np.array(all_bike_num_list)], pra_pref='bike')\n",
    "    WSADE,WSFDE = show_result(result_car,result_human,result_bike)\n",
    "    torch.save(model.state_dict(), model_save_path + \"epoch_\" + str(epoch) + \"_\" + str(round(WSADE, 4)) + \".pth\")\n",
    "    best_wsade.append(WSADE)\n",
    "    best_wsfde.append(WSFDE)\n",
    "    best_epoch.append(epoch)\n",
    "    epoch_end_time = time.perf_counter()\n",
    "    print(\"epoch spend time: %.4f\" %(epoch_end_time-epoch_start_time))\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    scheduler.step(WSADE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testLoader = Feeder(r\"/content/drive/MyDrive/data/Apolloscape/\", test_datasets, 0.8, 'test')\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testLoader,batch_size=1,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bestid = np.argmin(best_wsade) # index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7371586163838705"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_wsade[bestid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch[bestid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_save_path + \"epoch_\" + str(best_epoch[bestid]) + \"_\" + str(round(best_wsade[bestid], 4)) + \".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(r\"/root/data_apolloscape/epoch_179_0.7353.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inverse_transform_1(data, last_position,mean_xy):\n",
    "    # (64,6,115,2),(64,115,2),(64,2)\n",
    "    mean_xy = mean_xy.unsqueeze(dim=1)\n",
    "    last_position = last_position + mean_xy\n",
    "    for step in range(data.shape[1]):\n",
    "        data[:,step,:,:] = data[:,step,:,:] + last_position\n",
    "        last_position = data[:,step,:,:]\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_result1(prediction, origin,text):\n",
    "    # prediction:(1,6,115,2), origin:(1,1,115,3)\n",
    "    with open(\"/root/data_apolloscape/prediction_result/prediction_result/prediction_result_\"+text+\".txt\", 'a') as writer:\n",
    "        for step in range(prediction.shape[1]):\n",
    "            idx = torch.where(origin[0,0,:,0]!=0)[0]\n",
    "            step_info = prediction[0,step][idx]\n",
    "            front = origin[0,0][idx]\n",
    "            front[:,0] = front[:,0] + step + 1\n",
    "            all_info = torch.cat([front,step_info],dim=1)\n",
    "            for i in range(all_info.shape[0]):\n",
    "                a = str(int(all_info[i,0])) + \" \" + str(int(all_info[i,1])) + \" \" + str(int(all_info[i,2]))+ \" \" + str(float(all_info[i,3]))+ \" \" + str(float(all_info[i,4]))+ \"\\n\"\n",
    "                writer.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"no_pe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, batch_data in enumerate(test_loader):\n",
    "    features, masks,origin,distance_adj,heading_adj,mean_xy = batch_data\n",
    "    masks = masks.to(device)\n",
    "    features = features.to(device)\n",
    "    mean_xy = mean_xy.to(device)  # (64,6,114,1)\n",
    "    origin = origin.to(device)\n",
    "    heading_adj = heading_adj.to(device)  # (64,6,115,115)\n",
    "    distance_adj = distance_adj.to(device)  # (64,6,115,115)\n",
    "    # category_adj = category_adj.to(device)\n",
    "    last_position = features[:,history_frames - 1,:,-2:]\n",
    "    with torch.no_grad():\n",
    "        prediction = model(features[:,:,:,:6],distance_adj,heading_adj)  # (64,6,114,2)\n",
    "    prediction = inverse_transform_1(prediction, last_position,mean_xy)\n",
    "    save_result1(prediction, origin,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer=2:269\n",
    "# layer=4:270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# heading_adj_ = heading_adj[0,-1,:15,:15]\n",
    "# #heading_adj_ = heading_adj_[:,[0,1,2,3,6,7,9,11,12,13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# distance_adj_ = distance_adj[0,-1,[0,1,2,3,6,7,9,11,12,13]]\n",
    "# #distance_adj_ = distance_adj_[:,[0,1,2,3,6,7,9,11,12,13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aaa = (gs_attention[0] + gs_attention[1] + gs_attention[2] + gs_attention[3] + gs_attention[4] + gs_attention[5])/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gs_attention_ = aaa[0,-1,:,:14,:14]\n",
    "# #gs_attention_ = gs_attention_[:,:,[0,1,2,3,6,7,9,11,12,13]]\n",
    "# gs_attention_ = torch.sum(gs_attention_,dim=0)/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight_matrix = np.array(gs_attention_.cpu())\n",
    "\n",
    "# # 绘制热力图\n",
    "# plt.imshow(weight_matrix, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "# # 添加颜色条\n",
    "# plt.colorbar()\n",
    "# # for i in range(11):\n",
    "# #     for j in range(11):\n",
    "# #         plt.text(j, i, f'{weight_matrix[i, j]:.4f}', ha='center', va='center', color='black',fontsize=6)\n",
    "# # 显示图形\n",
    "# plt.title('GS_Attention Weight Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fps_attention_ = fps_attention[0,:,:15,:15]\n",
    "# # fps_attention_ = fps_attention_[:,:,[0,1,2,3,6,7,9,11,12,13]]\n",
    "# fps_attention_ = torch.sum(fps_attention_,dim=0)/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight_matrix = np.array(fps_attention_.cpu())\n",
    "\n",
    "# # 绘制热力图\n",
    "# plt.imshow(weight_matrix, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "# # 添加颜色条\n",
    "# plt.colorbar()\n",
    "# # for i in range(11):\n",
    "# #     for j in range(11):\n",
    "# #         plt.text(j, i, f'{weight_matrix[i, j]:.4f}', ha='center', va='center', color='black',fontsize=6)\n",
    "# # 显示图形\n",
    "# plt.title('heading Weight Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight_matrix = np.array(distance_adj_.cpu())\n",
    "\n",
    "# # 绘制热力图\n",
    "# plt.imshow(weight_matrix, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "# # 添加颜色条\n",
    "# plt.colorbar()\n",
    "# # for i in range(11):\n",
    "# #     for j in range(11):\n",
    "# #         plt.text(j, i, f'{weight_matrix[i, j]:.4f}', ha='center', va='center', color='black',fontsize=6)\n",
    "# # 显示图形\n",
    "# plt.title('distance Weight Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight_matrix = np.array(gs_attention_.cpu())\n",
    "\n",
    "# # 绘制热力图\n",
    "# plt.imshow(weight_matrix, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "# # 添加颜色条\n",
    "# plt.colorbar()\n",
    "# # for i in range(11):\n",
    "# #     for j in range(11):\n",
    "# #         plt.text(j, i, f'{weight_matrix[i, j]:.4f}', ha='center', va='center', color='black',fontsize=6)\n",
    "# # 显示图形\n",
    "# plt.title('GS_Attention Weight Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight_matrix = np.array(fps_attention_.cpu())\n",
    "\n",
    "# # 绘制热力图\n",
    "# plt.imshow(weight_matrix, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "# # 添加颜色条\n",
    "# plt.colorbar()\n",
    "# # for i in range(11):\n",
    "# #     for j in range(11):\n",
    "# #         plt.text(j, i, f'{weight_matrix[i, j]:.4f}', ha='center', va='center', color='black',fontsize=6)\n",
    "# # 显示图形\n",
    "# plt.title('FPS_Attention Weight Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
